Deepseek multi-head latent attention

<https://www.youtube.com/watch?v=jaZgLWtIVkE>

[[!toc levels=3 ]]

# Introduction

All right, everyone. So tonight we have a presentation. I found this awesome presentation out on the Internet on multi-head latent attention. So I thought maybe we'd give it a play. So with that, if you have any questions, so this is going to be this is our deeper dive on DeepSeek. And one of the key technologies was this multi-head latent attention. It's actually a way of kind of compressing the KV cache, which I think only helps during inference, but it greatly reduces the cache, like a factor of 15 or something like that reduction in the KV cache size. And we invited the speaker to join us today. Yeah. Yeah. So let me press share and share that. Okay. No, not entire screen. How about that window? Sorry, guys. Should be. The dramatic suspense mounts. Yeah, oh, I see. It's way down here at the bottom someplace. Okay, is that coming up? Do you get the video? Yeah, it says multi-head latent attention from DeepSeek-V2. Yep, all right. And then give me a thumbs up if you're getting sound, okay? And we'll get started.

# Overview of DeepSeek-V2

All right, awesome. So today I'm going to talk about this one particular architectural innovation from this DeepSeek-V2 paper. So we're not going to cover the entire DeepSeek-V2 model or the entire paper, but I've been on a little bit of a kick lately looking at LLM inference and optimizations and stuff, and they do something that's pretty interesting. And so I think this bears paying more attention. We'll see whether or not other people really pick up on this if it turns out to have legs or not.

So they created something that they call multi-head latent attention. And so DeepSeek-V2, they a company in China and they had DeepSeek version one, right? And so this is the DeepSeek-V2 paper from June. And because everything these days is moving so crazy fast, I think this past week, DeepSeek 2.5 actually just got released. So like before I could even do a paper on it, they already have a replacement for it, which is, it's not necessarily that big of a change from V2, but anyway, again, that just shows.

So we're not going to talk about the whole model, but just to give you a sense for it, it's big. It's 236 billion parameters. If you're familiar with Mixtral, it is using mixture of experts. So it's not just a fixed 200 billion parameters. They trained it on a very large corpus, over 8 trillion tokens. And so they have this mixture of experts that they use. If you're familiar with Mixtral, it did whatever, like two of eight or something like this. And they had a more aggressive, so their 200 billion parameter model, they had some small number of fixed shared experts. So these MLPs, like a regular LLM, are always on. And then for the switched ones, instead of having just like eight or something like that, they had a number like 100 or something like that. So it was like two plus six of 100 or something like that. So a very aggressive, I'm sure they must have done a lot of experiments to figure out how to fine tune, you know, the experts part.

It's long context, right? So that's definitely what everybody's asking for. You have long context, 128K. I don't know these days if that's not as newsworthy. It's not considered quite as long or whatever. And then not only using KV caching, which I've kind of talked about the last two months, I think. But then they implemented this thing that they created called multi-head latent attention. And that's what we're gonna talk about today is just this attention mechanism that they do differently from most transformers.

This is what Ivy League universities you can get into with a 3.6 GPA. 90.

And so this MLA, multi-head latent attention, the reason why their KV cache on their 200 billion parameter model they say is 15 times smaller than just a regular vanilla model. And that allows them to actually generate tokens 5.7, so a little short of six times faster than a normal LLM. And six times is a pretty big speed up.

Hey dad, a question. Is it the size of the KV cache or the access time for the KV cache that is the greater issue or are they just both big issues?

It's the access time ultimately and when we talk about like computer cache is often we think about like how big is it right but in this case you're only loading as many tokens as you have in your context so for a very short question what's the capital of France the capital of France is Paris that's so short that it doesn't really matter but if you're a hundred thousand tokens into an explanation of linear algebra then to output the hundred thousand first token you don't want to recalculate all the keys for the first 100,000. You're going to cache those. But you're still processing this in batches, and so likely you're not in single user mode so you can't just keep them there. And so you basically need to do this work of physically loading them onto your GPU.

Oh, I see. So when you're batching them, then the load to the GPU, smaller size is helpful.

Yeah, I guess I always thought it was an issue about GPU HBM size because it's not just the hundred thousand tokens it's that times the multipliers for the number of layers times the number of hidden state size it's not quite that but yeah so depending on how much you have on one GPU if the model's humongous you actually probably only have one layer per GPU. But nevertheless, in grand total, you are caching for every layer. If your key size is 64 floats, right? If those are 16-bit floats, then it's two bytes times 64, right? 128 bytes, you know, per token times if your context is 100,000. Now it's 12.8 million bytes of keys in a vanilla non-latent implementation right that's a certain amount of information that you just have to get from high bandwidth memory into SRAM.

Yeah, okay thank you reduces the size of the keys and values that they save there has to be some catch right you can't just magically shrink things by a factor of 15 or 10 or 7 or whatever.

So what ends up happening is if latent, they have to then sort of expand it out back into the full sized keys and values. And so there is a little bit of operations that are needed to do that. But if you're familiar with things like flash attention and I think it's now on like flash attention version 3 or whatever, what we've seen lately is that if you want to make your model run really fast, for the most part, what you're optimizing is minimizing GPU memory loads, you're not actually minimizing FLOPs. So, all the variants of flash attention actually do more matrix multiplies. They increase the number of FLOPs, but what they do is they decrease the number of memory loads on the GPU. And because a memory load is like 100 times slower than a matrix multiply, you can easily afford to increase your matrix multiplies by 10%. And then your model is going to run a lot faster if you can decrease your memory pressure. So, I haven't said how they do this reduction, but that's just sort a high level overview of what we're going to talk about here. Is there any questions from anybody yet?

# The KV Cache Bottleneck During Inference

All right. So what is this multi-head latent attention so we'll go into the details but high level all right so when you do inference on long context um having a KV cache where you save your keys and values is faster than recomputing the keys and values every time you're generating a new token so if you say you know uh what is the definition of of i don't know whatever uh uh an eigenvector and then you know your your model spits out and eigenvector is right every single one of those you need to uh attention needs to look at all the context all of the the tokens that came before so you're not going to recompute those you're going to cache them but even though this is faster this still becomes the bottleneck okay so then it's just a question of how fast can you literally copy can you load these keys and values onto your GPU um and so the smaller your your keys and values are the faster you can actually load them and that's where they get their speed up is this trick we're going to talk about.

# Prior Approaches: Multi-Query and Grouped Query Attention

All right. So what are some of the prior approaches? What have people done before for reducing the KV cache? So a very simple thing you can do is multi-query attention. So you basically say, I have 16 attention heads in every layer. I have 16-way multi-headed attention. I'm going to have 16 queries.

Could you maybe, people that may not know, just define what the key contains?

You're muted. Sorry. Yeah. So in our Transformers, we have attention layers. We have just fully connected MLP layers. In the attention layers, the conventional wisdom is that attention layers copy information from other token positions to the current token position and that the MLP layers do calculations on the current token position. So if you have, say, for example, a pronoun and you have the word he, and you know that earlier in the sentence you said the subject was John, then information about John can get copied from the token position where John is to the position where he is.

How do you do all that? So there's the attention calculation that decides which earlier token positions am I going to attend to. And obviously, in this case, I'm just making up that we know that a particular head is copying information for pronouns from their antecedents, right? There a lot that we don't know exactly what they're doing and they might be doing a little bit of a mix of different things. But the bottom line is what we do know is there a calculation that says which earlier positions are you going to copy information from. And typically there is like one that will dominate, but it's not going to typically be like eight of them that each get one eighth. There typically will be one that gets the majority of the attention. So that's what we mean when we colloquially say that we're attending to that earlier word, that earlier token.

So how that calculation works, if people are familiar, is that you calculate a query vector for the current position, and you calculate a key vector for every previous position that you might possibly want to attend to. And we take the dot product of this query and keys, and we run it through softmax to see which has the greatest, closest to one dot product. So those are the keys that are derived from all the previous words. And since they're just using a matrix multiply to derive the queries and keys, you can imagine a common thing might be that we're extracting some subspace. So there might be a portion of the token embedding that tells us if it's a noun, a portion of the token embedding that tells us it's a pronoun. If it's a verb, if it means something good, something bad, something big, something small, all those things might be embedded. And so with a matrix multiply, if you have a bunch of zeros, you can wipe out a lot of those other directions and you can pick out the ones that you're particularly interested in. And so if you say, I'm just looking for nounness and I'm just looking for in the key and pronounness in the query, then dotting those two together would get you a nice large number. Thanks.

But I'm only going to have one value that all 16 of them are. That's going to reduce the amount of memory I need to load from the KV cache by a factor of 16. That's going to be awesome. It's going to be so much faster. The problem is, we're going to see on the next slide, that it also doesn't work as well. And so your LLM performance drops.

So then people said okay well why don't we do something in between instead of dropping it all the way down to one I could cut it in half or cut it in quarters so I do 16 way attention and maybe I have eight keys and values and each key and value being shared by two queries so you have a diagram down below where where you can see like this is not 16 but if you had 16 16 attention you have 16 queries They all using one key and then once you calculate attention they're going to have one value. Or you could do something like this where you say, okay, I'm not going to be quite so aggressive and reduce it down to one.

So in this paper, they actually demonstrate some benchmarks that they did. Not going to go into, you know, too much detail, but these are not even terribly difficult benchmarks. MMLU, language understanding, you can see here that the regular full attention, which they abbreviate multi-head attention, MHA, the regular full attention gets significantly better scores than when they do a grouped query or the more extreme thing where they only have a single. So they were motivated to say, we want to reduce the number of keys and values, but we don't want to take this performance hit. So what can we do instead?

Actually, it's reduce the size, the total memory requirements, the keys and values, not actually reduce the number of keys and values, right?

That's right. Ultimately, you can make them individually smaller or you can reduce the number of them. Either way, we'll reduce your memory. Yep. Thanks, Roger.

Is it the reduction is just for performance rationale or why else would you do that?

It's just literally how many milliseconds, microseconds does it take you to copy those keys from GPU memory to SRAM in order to then do the dot product and the softmax. Oh, OK. So it's the actual operations, your mathematical operations are going to do. Well, copying to prep for the mathematical operations. That's right. So it's just literally if you have two megabytes of keys that you need to copy, it's going to take twice as long to copy as if you have one megabyte worth of keys. Gotcha. You know. And then obviously the math takes a certain amount of time. So cutting it from two to one is not going to double the speed of things, right? Because you still have other math and other overhead. But nevertheless, this is the bottleneck when doing inference in a vanilla attention, in a large model, once the context gets larger.

So obviously, again, at the very beginning, when you have a very short context, if your question is, what's the capital of France, the capital is Paris, that a total of like 20 tokens no problem You not bottlenecked But if you start saying I have this really really long context let say you doing some kind of RAG So I want to put in 32 top hits from my database. And then I want the LLM to decide amongst that where's the best text to answer the user's question. Now I've got 100,000 tokens worth of stuff that I've put in there. I can do like pre-fill and all that. And so, yeah, I've got it all. But even when you just generate your short answer of, you know, from all this text, the top three sentence summary I would give you is blah. For every one of those tokens that you output, you have 100,000 keys that you keep needing to load into SRAM. Got it. Thanks.

So the idea here is that you can't reduce the number total of keys and values. Let's keep the number of keys and values, but just find a way to compress them.

# Core Idea of Multi-Head Latent Attention (MLA): Low-Rank Compression

How can we compress them? Well, they ended up with a very simple technique. They're just doing a matrix multiply. So let's say that your keys and values are originally length 512. So you multiply it by a 512 by 128 matrix, and then your output's going to be only length 128. It's going to be four times shorter. What if we did multiply it by a learned 512 by 64? Now it's going to be eight times shorter. What if what if and so they tried different lengths and they were able to reduce it by like around a factor of 16, 20, something like that and still preserve a lot of the information.

So the idea is that if you think about these vectors they have a lot of floating point numbers there's 512 floating-point numbers. But hypothetically, if the whole vector was 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, just repeating, you really only need two numbers to represent that because it's just rank 2. So you could actually multiply that by a 512 by 2 matrix and then to reconstruct it you multiply it by a 2 by 512 matrix and you would be able to exactly reconstruct so the question is what is the intrinsic dimensionality of this information and I think this is interesting because a lot of people already already I think know is the right word although I haven't read the literature they know that the keys in particular are are a very low intrinsic dimension i'm not 100 sure about the values i haven't read as much stuff about the values but if you guys are familiar with the curse of dimensionality okay um when you have things that are high dimensional and by high i'll say like anything over 10 but we're usually way beyond that we're like in the hundreds um uh then you get these things that just do not make intuitive Euclidean sense. So like almost all vectors are super close to orthogonal. The dot product between any two vectors is very close to zero. Another thing that happens in high dimensional space is that the length, the distance between any two vectors ends up being very similar for any two vectors, okay?

But the whole point of attention is that you dot your queries and your keys, and you're supposed to get different values so that some of them have high values, and those are the ones we say we're going to attend to. And then you're going to have some low values, and you say we're going to ignore those tokens. And then the whole softmax thing is just a scaling thing so that everything adds up to one, okay? But if you had full rank vectors that were length 512, what would tend to happen is almost all of your attention scores would be similar. You wouldn't actually get really good differentiation where you're like, oh, all the attention is on boy, and there's very little attention on the word walk. Okay, so that's how it's intuitive for me, and I just haven't found where in the literature I think people have said this, but so we kind of already knew that...

Roger, can you pause? Yeah. So I want to make a clarification from that comment. So you're not... Earlier I said that like, hey, you would just have all these really low values. Another way of thinking about it perhaps more accurately is they not all going to be zero because you designed the keys to match with queries for certain reasons OK but what you would expect then is that where you have a match you would have this really high number and that all the other tokens would be really close to zero OK, but what we actually see when we do the softmax is not one that has a value of 0.99 coming out of the softmax and everything else is 0.001. Now, we actually see quite a few things have these intermediate values. And they could have intermediate values for multiple reasons. One is that they have some correlation with whatever you're trying to match on. So something could be slightly noun-ish if what you're looking for is nouns. That's one possible explanation. But another possible explanation is that it's a random vector that doesn't match what you care about, But it's just a random vector that happens to have a little bit of overlap, and it's not completely orthogonal. And if that were the explanation, that explanation only works in low dimensions. Once you get above 10 dimensions, a random vector that is measuring bigness, blueness, maleness, femaleness, the odds statistically of it having anything above a dot product of like 0.01 are extremely, extremely small. And so we would just see a much more pointy distribution where things that match match and everything else should be really, really tiny. And that's not what we see. We see intermediate values. And so the intuition is that one explanation, likely explanation, is that they're actually living in a relatively low dimensional subspace.

So is that observation before they tried this MLA? Did they observe that?

If you just off the shelf look at any model, Llama, whatever, whatever, the attention scores are not 0.999, 0.01, 0.01, 0.01. That's not what they look like. You actually see attention scores that are much more in between. Okay. Interesting.

Does that explain the high error rates of some models because it just can't pick the best one to complete or whatever?

I think we could go off on a long tangent. I think generally speaking, what ends up happening is heads are actually doing multiple things, composited. So attention head one in layer five right It might be matching pronouns with nouns when the current word when the key is a pronoun But what do you do the other 95% of the time when the current word's not a pronoun? You wouldn't want this attention head to be completely idle. That would be a very waste of compute power. So you can say when the current word's not a pronoun, then I'm going to do this other thing where I'm going to match verbs with adverbs. And I'm going to do this other thing where I'm going to match male things with other male things or whatever, you know, kinds of things it needs to do. And so when you superimpose all of this, then you also get the possibility that you get some of these intermediate attention scores because it's like, well, actually, it was some of that verb adverb thing that was triggering and not just the pronoun noun thing. So there are a bunch of different explanations, but the bottom line is that the conventional wisdom, sorry, I can't really cite you like the papers or whatever, but the conventional wisdom is that it's not this very high dimensional space that the tokens live in a very high dimensional space. But once you multiply it by that matrix in order to extract a query and a key for a particular head, that that query and key are living in a very different space and that it's probably fairly low dimensional. And then if you didn't believe me, ultimately the fact that latent attention works is a sign that they were able to reduce the dimensionality of that query key space by a factor of about 16 and not suffer any performance penalty.

Maybe you covered this already, but when you were talking about it has nothing else to do, so it's looking at relationships. Is that where the word latent comes from?

No, no. So I'm talking about just vanilla attention. Attention heads, the conventional wisdom is that they are doing multiple things at once. And ideally, the best way in which you can pack multiple things is if you have mutually exclusive situations. So if something cannot be a pronoun and be a verb then that attention head can have a pattern that fires for pronouns and a pattern that fires for verbs and they won't interfere with each other. In a perfect world, everything's completely orthogonal in that sense. I don't think, in fact, that LLMs have the luxury of having all completely perfectly orthogonal use cases. But nevertheless, the idea is that attention heads or doing multiple things. What we're going to do now is we're going to say, we have this key, it's like, say, 64 floats. And if we can compress it to four floats in a different space, that length four is the latent. And we will then take that latent. And before we actually do the actual attention calculation, we will project it back up. We will reconstruct it.

Thank you. I had an interesting thing there is when these, when the matrices for the weights for the query key value are initialized, they're going to be random. So these things would initially begin in a high dimensional space. There's something then that's collapsing those down into these lower dimensions. Is that, you're thinking that reuse is the thing that is pulling them together? Is it so if you have the luxury of having enough attention heads okay uh if you just had free compute and dimensionality and you said oh in in each layer i'm going to have 500,000 attention heads then they would probably all be single use and they would probably rank one like all i care about is is this a pronoun and the key all i care about is is this a noun that could be the antecedent so they're probably rank one at that point and just extracting like one dimension out of the embeddings because they are superimposed they're probably not all the way down to rank one but that seems like a sort of just you know logical conclusion that like if you had enough space and compute then you could just specialize each one to look for one very thing that it calculates accurately with no interference from any other information.

Hmm. Jan? Yeah. So, you know, there is the activation of the any other information. So there is the activated parameters, which is like a fraction of the total parameters. Does this discussion relate to that in any way?

That's a separate conversation. So that has to do just with the MLP layers and how basically we take one giant fully connected network and instead we make many, many copies and we only use a subset of them for any given token. I see. So nothing to do with the transformer layers. That's right. The sparseness really is only implemented in these mixture of expert LLMs, that sparseness is only today implemented in the MLP layers. Someday someone may figure out, but it appears that if you have a 48-layer model, you have only 48 opportunities to copy information from a different token position to the current position. And since we know that actually the first few in the last few layers are doing some specialized operations like, you know, New York City might be getting turned into a single, whatever you want to call it, pseudo word, right? And then the last few layers are doing some very specialized things. So the reality is you're down to about like 40 opportunities to transfer information somewhere else. So from a particular token position, you've figured out we're talking about the Eiffel Tower, and you have information about how tall it is, what year it was built, who the architect was, blah, blah, blah, blah, blah. You only have about 40 of these chances to copy that. And so I don't think we're ever going to see that much reduction in attention. Maybe you can cut it in half. You know, I don't know. But the MLPs, that's where we expect very sparse activations. If this MLP knows everything about the Eiffel Tower and it knows everything about a thousand other famous buildings and another 10 whatever bridges and all those things right In any one given sentence how many of those facts are you actually accessing A very tiny fraction So that you can design a switch that knows which one to activate was not necessarily a slam dunk. But anyway, that is another thing that MoE existed before and they figured out a way to make it much sparser we can talk about that in the end of this session or in a future session. It's another impressive engineering achievement. But this is not that. So it's crazy because we get confused. Like, wow, DeepSeek has so many things packed into it. Which innovation are we talking about here? This one is just the attention part. Got it. Thank you.

And Ted, is this actually in the V2 paper, the MLA?

Yes, yes. So it was introduced in V2 and it is largely unchanged in V3 and in 2.5. Okay.

And MLA is mostly helps with inference end of it, right?

Yeah, there's this one weird comment about the queries that I don't quite fully understand, but the main benefit for it is during inference, not during training. That's correct. And it's kind of interesting that they were thinking about inference performance at this time in development this is before they trained their big model right um yeah i mean v1 was 67 billion this is two something and now three is 671 or something like that but uh but yeah they still are kind of GPU starved so and and then you saw this is right it was like almost six times faster token generation that's that's pretty significant you know if you think about how tokens ChatGPT generates per day. Yeah, no, it's huge, huge. It's worth millions of dollars at that scale. Yeah.

Sorry, this might be a stupid question, but if you're saying the queries and keys, they occupy a lower dimensional space could you just have them a smaller dimension in the first place You know like if the values have you know as compared to the values and then instead of having them all the same dimension, or is that, have I not understood any?

# Why not attention in low-dimensional space?

Why not attention in low-dimensional space?

I think that's a good question. I think it's a good question. I had something similar, which was, could you not do the attention in the compressed space, but you're taking it one step further, which is just kind of, let's represent them in lower dimension space.

Yeah, I think both are really good questions, and I don't understand why not.

# Standard Multi-Head Attention Dimensions

So let's say that your tokens live in an 8,000 dimensional space. It's 8,000 floats, okay? And you multiply, well, so the convention is that for as many heads as you have, you divide up that space evenly amongst the attention heads.

So if you have 16 attention heads, 8,000 divided by 16, I think that's 512. So then you would have length 512 queries and length 512 keys.

Okay. So Alan's question is, well, instead of multiplying it by an 8,000 by 512 matrix, technically they do it slightly differently, but conceptually multiplying by an 8,000 by 512, and then using this latent attention method, multiply it by a 512 by whatever, 64.

Why don't you just use an 8000 by 64 in the first place?

Um, and I'm not really sure the answer to that.

And then Roger's question is like, okay well let's say you did multiply and you get it down to 64, why don't you just do attention on the 64s? Why do you have to do the attention on the 512s?

Yeah, because the projection is just a linear projection, right? So the math should be the same, should be identical.

# Why Low-Dimensional Attention Differs: Rank and Interference

There's a slight difference, Roger. If you have a rank-2 dimensional object in a—no, no, what am I trying to say?

If you have, if you say we've compressed it to where it's full rank, okay? So let me use easier numbers because big numbers are going to be harder for me.

So we started with 32 dimensional keys and we cranked it all the way down to two dimensional. Okay. Now we're just living in a plane.

Every vector is going to be—this is a full rank space and the queries, if they're two dimensional, then basically you're doing your dot product in this two dimensional space.

If you up-project, it's still rank two and it's still linear, but they can be living in different planes in that 32 dimensional space.

And so then vectors that are on the intersection along that line can still have a dot product of one, but you're not going to get the same potential interference from vectors that are not exactly collinear.

# Benefits of High-Dimensional Dot Products

So I do think that doing the dot product in the higher dimensional space gives you lower dot products for your non-matches, closer to zero numbers for your non-matches, because you have extra dimensions in which things cannot match.

Whereas if you have two full rank things and you get it low enough to under 10 dimensions, now you're no longer curse of dimensionality, now you're going to get a lot of interference from those other dimensions.

Does that make sense?

Yeah. So I don't think you can do the dot product in the lower dimension and get the same results, same kind of results.

If you want the ability—at least have the selectivity, the same selectivity.

# Multiple Subspaces Within a Head

Yeah. And especially if what we're seeing is, sorry, if we're doing multiple things with each head, then I think we potentially want the ability to say the condensed version is eight dimensions and it's two dimensions over here for dealing with pronouns and two dimensions over here for dealing with verbs and adverbs and another two dimensions over here dealing with, you know, blueness or whatever.

Right? So that's why when you expand back up you're not expanding them all together, they can be all sort of...

So what I'm saying is that the way the keys expand and the way that the queries expand can be different.

Okay. So that—so that.

Well, the queries don't expand, do they? It's the keys and the values.

No, no, no. I'm saying that when you expand the keys into the higher dimensionality, okay, if they go from full rank to this higher dimensional space, they can spread themselves out.

Okay, and the queries similarly, you don't want that. They don't have to. When they're in their full rank space, everything has to kind of collide, there's no room for them to just avoid each other.

# Per-Head Subspaces

I think if I have the mental model right, every query-key pair exists in its own subspace. So each head will have a different subspace.

Yeah, I'm talking about just within one head. Right. Right.

So, yeah. So, yeah.

# Flexibility in High Dimensions

I guess I'm struggling with this. If that space is a certain dimensionality, I'm going back to—I don't understand why you couldn't just do the multiply in that space.

I mean, you can do it and you might be able to get it to work. I'm just saying that the mathematical results will not be the same because in the higher dimensional space you can choose whether certain dimensions are orthogonal or not.

Okay, so I don't know if this would quite make sense, but like imagine you went just from two dimensions to three dimensions. Okay, so you have two planes.

You can choose whether to project those two planes like this, okay, so they only have a line in common, and everything else is going to give you a very zero-ish dot product.

Or you could actually have the two of them be more similar so that things that are not on that intersection line, they still could give you a dot product of like 0.8.

But in the full rank space, you have no ability to do that because you have no degrees of freedom to decide how you're going to orient things.

Now whether or not that's necessary, I can't make an argument, but yes.

# Exploring Low-Dimensional Alternatives

Yes, so I think both—I think Alan, I think Alan, you're—I think Alan your question was a good question though.

And yeah, yeah. If this is the case, somebody's going to explore this.

Yeah, and so one of the—yeah, I think that would be a good thing to explore.

# Learned Down and Up Projections

One of the things that's not clear to me is whether or not this down projection and up projection matrices—and by the way, they intentionally do not use the sort of pseudo-inverse of the down projection matrix for up projection. They both are learned and free to be separate.

Interesting. One possible explanation we'll see is that they're actually adding more parameters here. They're actually adding some compute power in here. And that might be part of what makes this latent attention work well. You're actually basically adding like another layer in there.

# Value Projections Remain Necessary

Yeah. It occurs to me that even if you were able to do something with making smaller keys and queries, you still would want to have a projection into bigger space for the values because they're being mapped into the token space.

Yes, the values you have no choice, the values live in the token space.

Yeah, the queries and keys live in their own separate little world and they could be something different.

Yeah, yeah, cool.

# Linear Projections and Final Thoughts

Is the projection non— or something completely linear?

Completely linear.

But yet in the small dimension they may not have that spread, but in the larger dimension they will. That's why I have it.

I'm not saying you cannot build something. That's the part I'm having a little trouble.

Yeah, same thing.

No, no, Roger's right. You can definitely build it in the smaller dimension. I'm just saying mathematically, you're not going to get the exact same dot product values.

So whether that really matters or not, I don't know. But, you know, if you just think about it, like appending a bunch of zeros, you're—you know, you're...

Yeah, I mean, I can imagine the other way. You know, if you go from higher dimension to lower dimension, you have some sort of loss. And therefore, I can imagine that it's possible, right?

So anyway, yeah.

# Queries in Latent Space and Dimensionality Limits

All right. why don't we keep going and then okay well we'll see there may be more questions at the end all right here we go queries had to be relatively low dimensional they have to kind of be less than 10 dimensional because if they're not you're not going to get such pointy peaked attention scores okay so so that's what they do and in this technique you know they found that they were actually able to compress these down without paying the kind of penalty that the multi-query and the grouped query techniques work. There is a slight caveat here, though, because mostly when I talk about attention, I just focus on the queries and the keys. But the reality is we have to have positional embeddings. The original Vaswani attention, you had embeddings that were added at the very bottom. So you embed each word into a dense vector, and then you add the embedding. These days, because of things like length extrapolation and other reasons, RoPE is very popular. RoPE a little easier to work with, because what you do is you just, you don't directly modify any of your token embeddings at any of the layers. But right before you do attention, you apply a rotation matrix. You do a matrix multiply, but it's a rotation matrix. And basically the number of degrees that you're rotating something depends on what the token position is. So for position one, you rotate it a little bit, for two twice as much for three three times as much for four times And then you do the same thing with the keys And so what ends up happening is when you rotate both of them then the only thing that matters in terms of how different they wind up being in the end is how far apart the query and the key are. So if they are one apart, they're both going to get rotated, but they're going to get rotated almost the exact same amount to each other. Whereas if one of them is very far, one of them is token one, the other is token 100, then the amount of rotations would be very different between the two. And so then the idea is that the transformer can learn to measure these rotation differences if it wants to know how far apart the query and the key are. So that's how it can measure that this word came five before my current word or something like that. So these rotations get applied right before you do the query and the key dot product multiplication.

# Handling RoPE in MLA

If you actually stored those in the cache, they had the problem that this would cause your keys and values to, your keys in particular, it would cause them to look very different because sort of the same information at token 10 is going to be different than the same information at token 100 because they have different rotations applied to them. And what they want is they want to like learn this pattern. They want it to be as low rank as possible, but you start adding these rotations and it's adding to sort of the complexity. It's adding to the rank of the information in the keys. So ultimately they had to come up with a trick where they sort of separately did rotation information, RoPE information, and they did base keys. And whereas in vanilla RoPE, you rotate the actual key. Instead, what they did is they said, well, just leave the key alone and we'll have some extra rotation information and we'll concatenate it. So then basically if the model wants to learn how to how far something is how far token is it'll just use this last part that has all the position information and if it wants to know the content of the word is it a noun is it a verb is it very masculine is it very blue is it very small whatever then it'll use the left hand side it'll use the the content portion.

Hey, Ted, a while back, we were talking about RoPE and you had made a comment, I think, at that point that as the query key values or weight matrices were learning kind of their projections, they must be kind of figuring out a way to separate their embedding from the positional embedding that's getting added in someplace and would otherwise mess it up. It's interesting because in this case, then you're back to keeping them separate, which means you don't have to store the positional embedding. You don't lose those dimensions in your key value cache or in your key cache, I guess. You get to add it back each time that you're going to do the attention.

Yeah. I mean, if you think that most of the operations in a transformer model are linear operations, then ultimately there has to be sort of like a separate subspace that's not being used for semantic information that the positional information can live in. And so even if you do the Vaswani addition of position, you know, the sine cosine thing, presumably those same dimensions where you're storing the sine and cosine information are not getting used for noun versus verb, because There would just be collisions where it's like, wait, I think this is a verb just because it's in token position five. You know, you can't have that. Right. So ultimately something has to happen. And so so this basically what they do is they say that I would imagine what they did was they just did experiments where they said, we'll do RoPE on the key. OK, but what if we make the key half size? What if we make it a quarter, an eighth? Like how small of a key do you need in order to actually maintain all of the information for RoPE to be meaningful up to 128,000? Yeah. And what they found was actually you don't need the full length of the sucker. You can actually have a pretty short one. And that's the thing that they're concatenating. But also it's you're not having to store it in its RoPE, its sine cosine manner. you're storing it just as a positional index in the sequence and re-adding it when you're constructing the key. I not sure if I understand what you saying They do apply RoPE to a key It just a mini key instead of the full key Okay Okay Now I had a different model I was assuming that they only add it to the key when they go to do attention and it didn't, they didn't have to store it, but I guess, yeah.

Wait one second. On the next slide, you'll, you'll see the diagram, I think. Is there a quick explanation of why RoPE? Why people like RoPE? Sorry? What's your question? Why people in general like RoPE? Why does RoPE help? What's the thing behind it that rotation causes? What's the advantage, I guess?

RoPE is good at relational um positions relative positions rather okay okay what what do you mean by relative roger so the distance with RoPE um if i'm so every token is getting rotated by a certain amount in many dimensions but let's just think of one dimension it gets rotated a certain amount um and it just increases as it goes along so the distance between token one and two is the same rotation as the distance between 10 and 11 and 12. Well, 11 and 12. Yeah. So it's, if you're wanting to query for something that's close to something that I know, then the relative position is easy to encode in RoPE. So if you think about a sentence where you say you know harry potter lived blah blah blah blah blah blah okay um what various tokens are going to attend to generally speaking most of the the the rules that i just make up i mean who knows if like my intuition is right but most of the rules are either going to be semantic where it's like oh this the verb for a sentence is going to look for the subject of the sentence you know um nouns and adjectives go together verbs and adverbs go together. But if they're positional, the most common thing would be something like the word Potter is looking for the word Harry that comes right before it. Okay. What do you mean by looking for It actually looking for something It just knows they pair together Let me give a better example Shoot I don remember the name of his uncles but they the Dursleys right so there's Petunia Dursley and the kid and the dad okay and so when you say whatever his name is Bob Dursley Vernon Uncle Vernon Dursley thank you thank you And Dursley. So if in the text you have blah, blah, blah, blank, Dursley, at that token position, you need to look at your previous token to see if it's Vernon or Petunia in order to know which person they're talking about. So it's a very common thing that Dursley is going to attend to the immediate prior token. And in fact, it's even more common because Dursley, isn't even a token in the Llama model. You know, it probably got broken up into three tokens, D, urz, and ley. And so it's a really common thing to look at the immediately one, two, three, however many preceding tokens. It's very rare that you would say, it is critical to me to know what was the seventh token in this passage, the absolute position. why would you really care what the seventh token is if somebody just said um um and it became the ninth token does that really change the meaning of the sentence right so um it's not that these models don't ever use absolute numbers but but that that's the intuition why RoPE would be particularly helpful because most of the calculations are going to be relative And like Roger said, it's really good at relative. So assuming you have enough floating point resolution, right, if you know what the sin of whatever, 128,000th of pi is, right, If you have enough floating player resolution to know what that is, then when you see that number, you know you're exactly one token away. And then if you see double that you two tokens away and so on and so forth And then eventually if you see whatever you know half of that then you know you 64 tokens away Right, right. But in reality, we are up in an N-dimensional space, right? Not in a plane or anything like that, right? It's kind of weird because, like, it's hard to do... I know it's hard to put a physical thing with it. And it's hard to talk about angles in higher dimensions. So in fact, RoPE actually just works in two-dimensional planes. So what RoPE does is just if you have a length 64 vector, it just does 32 pairs. And so they really are just the circular sine and cosine, the way you think of them. But there's 32 of them that I believe have different multipliers, right? So you can imagine one is like multiplied by one, one by two, one by four, one by eight, one by 16. And so they have different resolutions because of that. That's interesting.

Okay. Hey, Ted, do you know what size models they have released? The V2 was a 271 billion parameter model. Do they have any smaller ones?

I don't think so. I think it was just the single V2 at the time. Okay. And now they're still not released, ones you can play with. They distilled them into smaller models you can play with, but not released the actual model.

I don't know what you mean. The stuff that I played with on my machine, actually on Dave's machine, have been, you know, like the Llama 13B or whatever, which isn't the same architecture, right? It's just this, the DeepSeek model was distilled into the Llama.

Yeah, and I'm going to purposefully avoid the use of the word distilled to avoid any confusion. So they fine-tuned Llama 13B on 600,000 examples of long chains of thought. And it turns out that that is enough to cause any LLM architecture, including Llama 13B, to suddenly do really good chain of thought reasoning. Yep, yep. So my question was- cause any LLM architecture, including Llama 13B, to suddenly do really good chain of thought reasoning. Otherwise, it's still Llama. My question was, are there any scaled down versions of this DeepSeek model that are small enough you could play with? Like a Llama, you can get a 1.5B version, right? Is there a 1.5B equivalent of this DeepSeek model?

Yeah, I cannot speak authoritatively but my understanding is they did not build smaller models they just kept going bigger v1 v2 v3 kept getting bigger but understanding the desire for people to have smaller ones they can fit on a box yeah that's where they did that but to my knowledge there isn't a miniature small version that has the multi-head latent attention and all the bells whistles of the v3 but just yeah smaller size yeah that that was the reason my question i was wondering if we could play with some of this but yeah so so i think that is still a little bit of their technological mode right so that you basically if you're a chinese company and you want the performance of the full 671B, you may be going to them to host your inferencing.

Okay. Was there an earlier talk on RoPE? Yes. Okay. I'll go look that up. I had some more questions, so I'll just go look that up. Thanks. Just drop me a line if you have any other questions on it too. Okay, yeah, thanks. Thanks, Roger.

All right, yeah, so let's play, and then I'm pretty sure we're about to go to the next slide, which I don't think I talked about, but which does show a diagram so you can see how the whole thing gets put together.

# Detailed Mechanism and Diagram of MLA

Okay, here we go. Actually, when I first learned transformers, I just assumed that the positional bending was concatenated to the semantic information. I was very surprised when they're like, nah, we just sum the two vectors. I was like, what? Like, are there zeros in lots of places? And they're like, yeah, logically, but don't worry about it. It just works. And so this is actually kind of going back to my more simple intuitive thing of saying that instead of just summing them rotating them whatever saying that if you think about it dimensionally the model can separate out the two things Now as a human we can easily see the separation because we literally saying the first whatever 256 is the meaning part and the next, you know, 48 is the rotation part. So that's this extra trick. I didn't go into all the details of exactly how they do that. But so when you combine these, you get a diagram that looks something like this. So here is your input to the attention layer, okay, and they are going to multiply this by a matrix that's going to make this shorter. So this C that we're talking about on the left and right for the query and the key is going to be smaller than our original token. That's the whole point of this. They want this to be small, and in particular, the keys and values are going to go in your KV cache. I think maybe that's why it has these slashes on them because this is what actually goes in the kv cache oh yeah up here the legend cache if it has slashes so this thing is smaller a lot smaller it turns out like more than 10 times smaller and that's what we need to store in the cache and then ultimately logically we need to reconstruct the full length keys and values okay so there's another matrix multiply here then we can just do regular attention on keys and values where we dot product them, we do the softmax. That's conceptually not changing at all. But you'll notice that there's this little thing here that says, here's my regular key, and here's my rotation information for the key. And here's my regular query, and here's my rotation information for the query. So again, Vanilla RoPE would have modified this query and modified this key. But they did not want to do that because that affects their ability to cache stuff. And so instead, they concatenated these special rotation information.

Ted, here was the comment I was making earlier about not requiring to use dimensions in the key to store positional information because if I'm understanding this diagram correctly, the cached latent KV cache does not include positional information. It is added after you've up-projected it. So, if you think about it, you're not wasting the dimensions in this space for positional information.

So unfortunately I don't think that accurate If you look at the rotation part that in the dead center of the diagram it dark shaded because it is also cached along with the latent part that does not contain the rotation information. You're thinking it's cached as sine cosine information? So you take the token H that's at the very bottom. Yeah, yeah, yeah. You apply rotation to some small portion of it. They tried to diagrammatically show it as being smaller. And then it is used in attention, so it needs to get loaded into SRAM. So where is it going to come from? The two possibilities are you load the giant H and recalculate it. That's not a win because H is really long. so what you really would rather do is just cache it and load it itself you could yeah i mean you could also generate it on the fly based on that position sequence position but either way it's not it's not in this space it's not it's not in that space but but but you do need you either need the RoPE rotation matrices and or the H. You need those inputs in order to regenerate it. And the problem is all of those things are bigger. And so ultimately what you want to do is you want to just cache the little K-R thing that's in the middle. Okay. But your point is right. Any place where you can regenerate something, that's the fastest. But in this case, loading the things you need to regenerate it would be slower than loading the rotation input itself. Yeah, I guess I'm not understanding this line right here because it seems like it's just the position of this in the sequence. It's not generated from the hidden state. Yeah, it's not clear from the state. I actually misunderstood. At one point, I wasn't sure whether it was just a fixed rotation set of information, like universal, you know, token nine has this key. Yeah, yeah, yeah. That's right. But my latest understanding is that is derived from the token from the H So there a matrix multiply by H and then it rotated Okay. All right. Let me stop with that one then. Alan, you had your hand up?

Yeah. Sorry. I don't know if I misunderstood anything. I'm just wondering what this kind of latent kind of CTQ is. because I thought the kind of latency referred to kind of, you know, compressing the keys and values to cache them. Whereas with the queries, I didn't understand the relationship between that and caching.

Good call out. There's two things that I haven't really talked about. It was trying to just give a flavor for the concept. So one thing that may answer a little bit the question that Alan, you and Roger had asked earlier about, can you just use a lower dimensional space and whatnot, is there's this unusual thing that when we compress the key to a latent, we are actually compressing the keys and the values simultaneously. So somehow what they're saying was that they found a certain overlap of information between keys and values such that compressing them together was more valuable than compressing them individually, separately. And I think that also speaks to why you don't just change the key matrix to be smaller, because then that would be doing keys and values separately. So I think that's part of the answer there. The other thing is they also squish down the queries, which for the purpose of KV cache serves absolutely no purpose whatsoever. And the sentence that appears in the paper says that squishing the queries down reduces the size of activations somewhere in the model. And I didn't really fully understand that comment.

But was it one of those ablations where they tried it and it worked better and they couldn't figure out why and they kept it?

Was it one of those situations? Yeah. I haven't tried to work through it, but I think what happens is, you know, the latent's smaller than the full query, whatever, right? And so I believe what they're able to do is rather than store information at the full query level, they're able to store the information at the latent level. but if necessary they can always as Roger was pointing out they can always reconstruct the query if they need it as long as they just have the latent information so they operate with those shortened queries and the and the um you know sort of the back propagation the activation and gradient information at that shortened level uh on purpose just because it's smaller but that makes sense i haven't done the math to see that exactly how it works that makes a lot of sense When we do the dual pipe stuff, they talk a lot about what is the information, the state information that you need to keep, hold on to, to wait for your back propagation to come back down. And so I could really easily imagine that it's easier to store just your query in a small compressed space. And then as the back propagation comes through, you expand it out to do the math then. So that makes a lot of sense. Thanks for that. Yeah, yeah. There was another paper I read where there was multiple heads and the vanilla thing you do is you just sort of magically, you just sort of store everything, which they were saying that this actually like massively increases the amount of memory you need for backpropagation. And so they came up with a trick where instead of doing backpropagation the vanilla way, they would just do a tiny little bit of backpropagation on the heads and they would accumulate that intermediate value. And they found that that was actually enough so that instead of having to store them all separately, they could just store the accumulated value in the middle and then backpropagate that the rest of the way. And so they're like, yeah, this little trick actually saved us like a ton of GPU memory on the back propagation. Yeah. There's a real art there. We're trying to figure out what do you store versus what do you recompute? Yeah. Yeah, exactly. And that as I said earlier the flash attention thing is about storing what you can you know in SRAM and recalculating things instead of waiting to load them So yeah All right Let me hit the play Oh sorry Sorry

Yeah. So this is probably a silly question. The, the, you know, is it a split that they're showing where you have the input hidden, and then you have the latent CQ and the KR and the latent KV. What exactly are they trying to depict there? Are you talking about this little?

Yeah. Yeah. I think it's not a split. It's just a copy goes to compute query, a copy goes to compute the key and value. So it's projected into the space and then later projected back up into the the full dimensional space. And then this one, this one, I don't understand, but. So, so, so actually there are matrix multiplies that happen to calculate the things and they are not shown on this diagram. They'd be kind of on this line right here on those lines. And actually what I was saying, Roger, is that vertical middle line has a matrix multiply too. Yeah. And somehow the, and somehow the result is cached, right? The HT is not cached, but the result is cached. Right. So really, this diagram is not that different from vanilla attention. In vanilla attention, you do three matrix multiplies, the Q, the K, and the V. And the K and the V are cached. And kind of like on the right, you have that CKV that's cached. It's really in the same place that you cache them in a vanilla attention. The difference is that this CKV is 16 times smaller or something like that than the regular attention formula. So otherwise, in that sense, you still have a query on the left, a key and a value on the right, and they do their thing. OK, OK, thanks.

So Ted interesting thing Originally I thought then the compression of the KV cache was an inference benefit But it sounds like this is potentially a training benefit as well because of the need the ability to you know save state at one sixteenth of the size

Yeah if there a benefit on the query side there may also be a benefit on the key value side but i i honestly haven worked through the math in my head but there's a screamingly obvious inference benefit because just the shaded thing is what goes in the cache and has to be loaded into sram and that thing is way smaller yeah all right all right there's q a at the end of this video so i think we're actually close to the end of the presentation so i think we might be able to just finish just about here we go

# Storage Reduction and Performance Benchmarks

So here's the storage reduction. Basically, if you go through these formulas, if you L is the number of layers and D is the dimensionality of each head, like I was saying, like 512. So if you look, these two variables appear in every single formula. Okay. So you can basically just sort of ignore the number of layers and ignore the size of each head. And they're saying for the multi-query, we're going to store two things, one single key and one single value. Okay. For regular full attention, it's going to depend on the number of heads. So, if you have 16 heads, we're going to store 16 keys, 16 values. So, it's two times the number of heads. And if we do the grouped thing, then it depends on how much you're grouping. So, if you have eight groups, then that's going to be half as much as if you did fully everything. Okay. So the idea here is that let's just use 16-way multi-headed attention, okay? So the top row regular is going to be a 32 times the stuff on the right. The next row might be 16 times the stuff on the right. The bottom row is two times the stuff on the right. And this guy here is only 2.25 times the stuff on the right. So it's almost as small as the one. oh sorry sorry sorry it's it's four and a half it's two times bigger than the smallest one here the multi-query thanks for catching that um yeah so it's it's a little more than double this really really small one the multi-query where we only kept one key okay um so we're keeping a little more than the equivalent of two keys uh and then the question is right well how does perform Right So this is the part that I thought was a little odd I like that they said stronger Yeah What was that? You know, in the previous table, they just said stronger, stronger than what? Stronger than the others, right? Stronger than regular multi head attention is what yeah. And so the numbers on the next slide actually support it. It actually did better on benchmarks on all but one okay mla is stronger than mha yeah cool and again it may be because you have these extra parameters you actually are making your model bigger all right two times bigger than the smallest one here the multi-query thanks for catching that um yeah so it's it's a little more than double this really really small one the multi-query where we only kept one key Okay. So we're keeping a little more than the equivalent of two keys. And then the question is, right, well, how does it perform? Right. So this is the part that I thought was a little odd. They have this table. It's in the appendix. And what they said is, group query, you take a performance hit. Multi-query, you take a massive performance hit. They didn't just say we didn't take a performance hit. They actually said, actually, this exceeded the performance of full attention. We did better. Now, that's possible. It's possible that if they're sort of like promoting the right inductive biases or whatever, that by lowering the rank, it's actually helping the model learn faster and learn better relationships or whatever. But if this is true, I think this is like much more surprising than just the fact that, hey, we actually saved, you know, like whatever, 12 times, 15 times the amount of memory for our KV cache. I mean, that is impressive that the KV cache loads six times faster or whatever. But like, really? So then I would do this even if I didn't get the memory benefit if it's actually going to give me a better model. So one of the things I say is you should always be a little bit skeptical about results in papers. You mentioned like, hey, this one here was not a really great comparison because they actually compared it to a five-year-old really crappy model. Like, of course you beat it. Like, that's not really a fair comparison. Who knows if they cherry picked the because they actually compared it to a five-year-old really crappy model. because they actually compared it to a five-year-old really crappy model. Like, of course you beat it. Like, that's not really a fair comparison. Who knows if they cherry-picked these results, if maybe they had 12 benchmarks and their thing did worse on eight of them and they just only published the four that they did, right? So you always have to take it with a grain of salt. But nevertheless, this will potentially be a useful technique on two levels. Even if these results are cherry-picked and actually it only has 98% performance of regular full attention, that's probably good enough for a lot of use cases to get six times faster inference. I mean, right, like if you think about how many like billions of dollars OpenAI is losing every day because they're giving all the money to Microsoft for compute every time you use ChatGPT right if they could cut their compute costs by a factor of six that billions of dollars that this would save them okay So it useful it useful just just for that right it also useful if somehow this actually works better than attention whether you do or not but but so for all but the most intense things

# Conclusion and Optimizations

Basically, they reduced the size of the cache dramatically. Interestingly, they said they actually didn't just match, but outperformed it. And so their shorter vectors are about 1/7 the length of full length keys and values And there is some stuff I mentioned they had to do with RoPE There also is some things they can do where you have to do this matrix multiply to reconstruct the original vector from the short one. But it turns out that that matrix multiply that you do is either immediately preceded or immediately followed by another matrix multiply. And when you do more than one matrix multiply in a row, the matrix multiply is associative. And so you can actually multiply the two matrices, weight matrices together, instead of multiplying them by your data sequentially. And so just like there's this trick in LoRA, where you have this adapter thing, and you technically are supposed to sum it with your regular weights. LoRA says, but you don't have to do that at runtime. You can just pre-compute the sum of the two, and you can store that. What they saying is they actually figured out ways they could precompute the product of the two matrix multiplies and they could store that instead of having two separate weights and actually having two at runtime multiply them over again Bottom line is you can look at the paper if you want to see more of the details but the bottom line is that you would think that there a lot of extra compute to reconstruct the full length keys and values, and there actually isn't because they managed to fuse those operations into something else. There's still a little bit of extra stuff because remember there's those little RoPE position things we concatenated. So they still need to calculate those, but they didn't actually have as much extra overhead as you might vanilla implementation thing.

Yeah. So at the end of the day, this DeepSeek-V2 model, because of the heavy mixture of experts, it's only using 21 billion parameters during inference. Okay. They have the 236 billion parameters, but you're only, when you do a forward pass, you're only using 21 billion of them. And I didn't print it here, but they have performance that in many ways seems to sort of beat Qwen 1.5B to be comparable with Mixtral, which again, it's using 22 billion of them.

# Possible transcription errors

**Acronyms**: Standardized MHA (Multi-Head Attention), GQA (Grouped Query Attention), MQA (Multi-Query Attention), MLA (Multi-Head Latent Attention), RoPE (Rotary Position Embeddings), MoE (Mixture of Experts), RAG (Retrieval-Augmented Generation).

"Mixtrawl" → "Mixtral"; "Lama" → "Llama"; "CTQ" likely transcription error for "CQ" (latent query).

