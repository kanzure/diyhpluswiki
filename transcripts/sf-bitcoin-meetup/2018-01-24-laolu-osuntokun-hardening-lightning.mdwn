Laolu Osuntokun

Hardening Lightning - BPASE ’18

Video: https://www.youtube.com/watch?v=V3f4yYVCxpk

Slides: https://cyber.stanford.edu/bpase18

# Introduction

I’m going to go over ways that I’ve been thinking about hardening lightning in terms of making security better, making the client more scalable itself and addressing some issues and pain points that come up when dealing with fees on mainnet. There’s going to be some changes that I’m going to propose to Bitcoin but in my opinion they are relatively minor so I can see them having a pretty good chance of getting in. They’re not really big sweeping changes. One is a new SIGHASH type, one is a new OP code and then there’s covenants but that’s like a whole other story in itself. 

# Table of Contents

A quick overview first. I’m going to give an overview of lightning’s security model. I won’t really go into the mechanics of lightning itself as of now because I assume people know what that is vaguely. Payment channels, you connect them, you can route across them, that’s the gist of it. I’m going to talk about hardening the contract breach defense. Some people talk about ways you can do that in terms of adding more consensus changes. I’m going at it from a point of view of doing a different strategy when a breach actually happens in the face of a large backlog. Then I’ll go into a new channel type, a new channel design. We’re basically making channels more succinct meaning you have to store less history and also things like outsourcing it a lot more efficiently. Then I’m going to talk about outsourcing and a newer model that has minimized the client side as much as possible. If you want this to actually scale and the outsourcer to support a large number of clients then we basically want them to store as little state as possible. Then we’ll go into making lightning a little more succinct onchain because if it is an offchain protocol we want to ensure we have the smallest footprint onchain as possible. Otherwise it’s not really scaling because you’re hitting the chain every single time.

# Lightning’s Security Model

So first, the security model. There are these layers of lightning over here. People always say it is Layer 2 but to me there are a lot more layers on top of that. For me, Layer 1 is the blockchain itself: Bitcoin or whatever else. Layer 2 is the link layer between channels so this is basically how do I open channels between me and Bob and how do Bob and I update that channel. Layer 3 is the end-to-end routing so like HTLCs and end routing whatever else that is. Layer 4 is the application layer, where people are building on top of lightning things like exchanges and whatever. So basically the way lightning works is that it uses Bitcoin as a dispute mediation system. Rather than every single transaction going on Bitcoin itself, we basically do contract creation on Bitcoin itself and we do enforcement but the execution is offchain. We do the creation initially then we do all the execution on the side, that makes it really succinct and that’s fine. We basically treat the chain as this trust anchor. The way it works is that any time we have a dispute, we go to the blockchain with our evidence and with that evidence we can say “Hey. Bob was cheating me. He violated a clause in the contract” and then I get all the money back. We have the easy way or the hard way. Optimistically we can do everything offchain or if we want to exit the contract we do signatures between each other and boom it goes on chain and everything is fine. The hard way is in the case that I have to go onchain and either..  the contract myself or possibly go onchain to do a proper dispute because you violated something in the contract itself. When we’re handling disputes we basically say that we can write to the chain eventually. By eventually, this is configured by the time parameter called T which is the csv_value. This means that any time someone goes to chain there’s a contestation period and I have up until time T to refute their claim. We say eventually because T can be configured. We want to configure T based on the size of the channel. If it is $10 channel maybe you want a few hours. If it is a million dollars maybe you want a month or whatever else. The other thing we assume is that miners and pool operators aren’t colluding against us because otherwise they can censor all our transactions onchain. We also assume a level of decentralization of mining because otherwise you can get blacklisted but there are ways to make the channels blend in with everything else in the network. There’s some really cool work on that front itself.

# Hardening Contract Breach Defense (Strategy)

Something that comes up a lot is people asking how do I handle contract breach in the case of massive backlog. This is the case where for whatever reason fees are sky-high, someone is spamming the chain and then someone does a contract breach. What am I going to do from there on? There are some things that people have worked on in terms of adding new consensus changes to Bitcoin. Maybe something like the timelock would stop after the block was so full. Or possibly you’d have a pre-allocated arena where you can do barter for your smart contracts. This is looking from a more strategic standpoint, looking at the dynamics of the fees on the way we actually handle the commitment transaction itself. So whenever someone tries to broadcast a prior state, they’re basically locked into that particular state. If Bob has $2 on the new state and he had $10 on the other state, he’s going to go back to the $10 state. At that point, Bob can only access his money within the channel itself. But me, myself because he revoked all those outputs, I have a little more wriggle room. What I can do is progressively start to siphon Bob’s funds into miners’ fees. Now it’s a situation where there’s some strategy because Bob has two options. He can stop or I can keep going and I’m going to eventually siphon all his money into miners’ fees. The only way he will succeed in getting that prior transaction is if he pays more in miners’ fees than what’s in his balance. He can do that using pays-for-parent, that’s the only option really. I think this is a pretty good approach. What we’re doing here is that the justice transaction is going to be using replace-by-fee. We’re going to use that to progressively bump up and siphon the money of the cheater into fees. This is a pretty good stopgap in terms of the way things work right now. If you have someone that can be vigilant at all, you can almost always penalize this person. So even if there is a massive backlog, assuming the miner wants the higher fee rate then Bob gets nothing. So now there is even further incentive against people trying to do cheating because now there is this strategy where I can pull your money into miners’ fees and you get nothing. I’m still made whole this entire time. Whenever this happens, I’m ok, I want to punish Bob and this is me punishing him in the worst way possible. I’m going to basically jump to the front of the queue for everything else in the mempool. I could’ve put 20 BTC towards fees and I’m fine, I’m just punishing him. It is scorched earth, I wipe my hands and I walk away and everything is ok.

# Reducing Client Side History Storage

So now we’re going to talk about scaling lightning itself from the client side of it. Basically the way it works is that because contract execution is local, we have a local transcript. Every single time we do a new state update there is kind of like a shadow chain. By shadow chain I basically mean that we have kind of like a blockchain itself because the states refer to each other in a particular way but also because we can do state transitions between the states themselves. The shadow chain is only ever manifested on chain in the case of a contract breach or if I want to force close. Typically in the normal case we do a regular cooperative close which means we both sign the multisig, we go onchain and you see nothing else. The shadow chain is only manifested in the case of a breakdown or some type of breach. The state transition we have in the channel can be super generic, probably sometime later this year we’ll have something fancier - some cooler stuff. Right now, it is basically just adding new HTLCs, removing HTLCs and keeping track of that prior state itself. The goal here is to reduce the amount of state that the client needs to keep track of. This can be good because if they can keep track of less state they can have higher throughput changes but also this has implications for the outsourcer which I’ll get into a little bit later. If the state required for the client to act on the contract is reduced, that makes the… a little more succinct as well. If the… is succinct then people are going to run them. If people are going to run them we have more security. It is a laudable goal to go forward.

# Reducing Client Side History Storage - Overview of Commitment Invalidation

Before we get into this, we’re going to talk about the history of how you do commitment and validation on lightning. You have a series of states and you walk forward in these states one by one. Every single time we go towards a new state, we revoke the old one. That’s the state sequence. We were on State 1, we revoked it, State 2, we revoked it, now we’re on State 3. A central question on the way we do channels is how you do invalidation. One thing to note is that this only matters for bidirectional channels; when you’re basically going both ways. If it is a unidirectional channel, every single state update I do is benefitting the other participant in some way. You don’t really have an incentive to go back on the prior states. But with a bidirectional channel, there’s likely some point in history from where we are currently in which the other participant was better off. They have some incentive to try to cheat me and try to go back to that prior state. We solve this by invalidating every single state once we make a new one. The penalty here is that if I ever catch you broadcasting a prior state you basically gets slashed which means your money all goes away and that has some strategy. Naively you keep all the prior states but that’s not very good because now you have this linearly growing storage as you do these updates. People like lightning and different offchain protocols because they can be super fast but if you have to store a new state for every single update then that’s not very good. The other thing here is that you can go to the blockchain and close out to reset the prior state but that’s not very good because you now have all these control transactions on the chain which aren’t necessary to make everything succinct. 

# Reducing Client Side History Storage - History of Succinct Commitment Invalidation

Going back in the history of how we do commitment invalidation. So when channels first came along, the first bidirectional channels initially used the BIP 68 mechanism. BIP 68 is this kind of relative timelock thing. So we start on 30 days, then we do an update and now it goes to 29. We do an update and that goes to 28. The way this enforced the latest state is that you could only broadcast the latest state using the timelocks. If you had state 30 then I’m going to broadcast 28 before you can actually broadcast that because the timelock hasn’t expired yet. The drawback on this one is that this has limited number of updates so we can only do 29 updates or 30 updates or 50 updates or whatever else. We have to lock that into the lifetime of the channel. Going on, some people (Decker and some other people) made this thing called the invalidation tree. The thing with the invalidation tree is that you kept this decrementing timelock thing. Then you added another layer on top of this which is basically a tree layer. Because of the way the timelocks work you can’t broadcast the leaf before you can broadcast the root itself. Also because they’re bidirectional channels, they had to reset every now and then. They were constructed with two unidirectional channels and when the balances get out of whack we have to reset that. Every time we reset that, we have to decrement the timelock. Every time we decrement the timelock we have to measure this root thing. That could let you with a tweak, call it the kick-off transaction, that can let you have an indefinite lifetime channel which is pretty cool. The drawback of that is that you have this additional offchain footprint. So if you have this massive tree that you need to broadcast to get to your certain state, now it is not very succinct because you have to broadcast that entire state and we want to make it more succinct. What we do now is we use this commitment revocation thing. Basically the way it works is that every single state has a public key and when we go to the next state, you must give up that private key. It is a little more complicated than that but that’s how it works more or less. The cool part about this is that we figure out this way to generate the public keys in a deterministic way. Me as the client has a constant… state and I can do this tree type pseudorandom number generator thing to generate all these different secrets. You as the receiver can collapse these down into the particular state themselves. The goal here is to develop a commitment that has symmetric state. One thing about commitment revocations now in lightning is that the state is asymmetric and this is due to the way we ascribe blame. I know what my transaction looks like and you know what your transaction looks like so therefore if you broadcast it onchain, I have what I need. That can get… in terms of multiparty channels which are pretty cool. If we can make them symmetric, multiparty channels wouldn’t have this weird combinatory state blowup and also we can make all the state much smaller itself.

# Reducing Client Side History Storage - Commitment Invalidation Lightning 1.1

So here we review the way we do revocation right now in lightning. I’ve put 1.1, I don’t know, I was thinking ahead basically. The way it works is that every single state has this thing called a commitment point. A commitment point itself is just like a EC basepoint. The way we derive the private keys for each of these points is this thing called shachain. So shachain, you can think of it, you basically have a key k and an index i which can give you this random element in i. Me as the receiver, because it has this particular structure, I can collapse them. Anytime I have shachain element number 10, I can forget everything else because I can rederive everything else from shachain 10, more or less. We do this key derivation which is kind of complex but the important takeaway here is that when we do this state update, you give me a public key and then I do some EC math where it turns out that once I reveal the private key for this thing, only you can actually sign for that state. Basically we make one of these commitment points and the revocation point, we put it in this new commitment. When we go to the next state, you must reveal that to me and that’s how it works. This one has a few drawbacks. One thing is that it kind of gets involved because we’re trying to defend against rogue key attacks, things like that. I think we can get even simpler. One of the drawbacks to this one is that the client storage, he has to store the current state but then he has log(k) state where k is the number of state updates ever. The outsourcer needs a signature for every single state and in addition to that it needs a signature for any other HTLC we have. It also needs to collapse this log(k) data itself. The goal is to make it a little bit simpler, make it more succinct.

# Reducing Client Side History Storage - OP_CHECKSIGFROMSTACK

So now a diversion. I’m proposing an addition to Bitcoin called OP_CHECKSIGFROMSTACK. Bitcoin checks signatures, you probably have millions of signatures when you’re validating the blockchain itself. The message is always assumed to be this thing called the sighash. Any time there is a signature operation, implicitly we generate this thing called the sighash which is derived from the transaction itself and it can control what’s being signed. That’s cool but it’s a little bit restrictive. What if we can add the ability to validate signatures on arbitrary messages? This is like super, super powerful because this can let you do things like delegation. I could say “Hey. Here is Bob’s public key. It is signed by mine therefore we can take this output.” We can do things like oracles. We can say “This is an integer signed by Bitfinex and we’re going to use it for some contract.” We can also have these blessed message structures where your protocol has a message that is opaque but has a particular structure. This can only be signed by both participants. This acts as evidence that we actually signed this at some point in the future and we can use this as evidence later on. This is kind of like a strawman proposal but it is not super soft fork safe or Bitcoin-y or anything like that. But basically you have a message, a signature, a public key, maybe you have a version, maybe you’d have different signatures in the future - ECDSA or Schnorr or whatever else. It tells you whether it is valid. With Bitcoin, you’d have a OP_DROP and all this other stuff but we’re doing this for simplicity for now.

# Reducing Client Side History Storage - Signed Sequence Commitments

Now to the new commitment and validation. This thing called signed sequence commitments. Rather than using this revocation model, what we do is every single state has a state number. We then commit to that state number and we sign that commitment. That signed commitment goes into the script itself. This is cool because we have this random number r so when you look at the script you don’t really know what script we’re on. But in order to do the revocation, what we do is we say “Hey. If you can open this commitment itself - because it is signed you can’t really forge it as it is a 2-of-2 multisig - if you can open the commitment and then show me another commitment opening whose r sequence number is greater than the one in this commitment, then that means there was some point in history where two of you cooperated to sign this new state and you went back to the prior state to try to cheat me. This is a little bit simpler. What we have is a signed sequence number, you prove to me that there’s a new sequence number and we commit to it because the hide the state of it. It could be the case that when we go to the chain we don’t necessarily want to reveal to the participants how many states we’ve done. Signing is pretty simple. You basically have this number r which you can derive from some deterministic method. We increment the state number. We have c and then the signature is important because it is an aggregate signature between both of us. There are a few techniques you can do for this. You can do two party ECDSA, there’s some aggregated signature stuff we’re going to be talking about tomorrow. Somehow we collaborate and do this signature and it… a lot easier. One cool thing about this is that now whenever I have state 10, I don’t need 9,8,7,6,5 or any of the other extraneous state because 10 is greater than 9 which is greater than everything else. This means that I now have constant client storage which is pretty cool. There can be like a million different states and I only need to have this one four-tuple thing: the signature, the commitment and then the opening of a new commitment itself and that’s all I need. That’s pretty cool because now the outsourcers themselves, they have constant size state per client. Before it would grow by some factor of the size of the history but now it is constant which is a pretty big deal. Also it is a little bit simpler in terms of the key derivation, the script, things like that because we’re just checking the signature. The cool thing is that there is a state machine in BOLT 2 which is how we update the channel states. That stays exactly the same. We don’t need to change anything fundamentally. We’ve dramatically simplified the state and the cool thing here, which I’ll get into later, is because the state is symmetric now, in multiparty channels there’s no longer this combinatory blowup of who published and what was their state number when I published and who spent from that… it gets way simpler with this stuff which is pretty cool.

# Scaling Outsourcing - Review of State Outsourcing

Now we’re going to get into how this makes outsourcing more scalable. Outsourcing is used when a client may not necessarily be able to watch the chain for themselves. We have this time-based parameter T. If I’m off-line and the outsourcer detects this change/event onchain between this period of time T, they can punish the other participant. The cool part is that because we can outsource this, we can have light clients, mobile clients, whatever else. In the prior model, I’ll talk about the new design later, we send them these initial basepoints which allow them to rederive the redeem script itself because when they see a transaction onchain, we actually encode the state number of the commitment in the sequence number of the locktime. The outsourcer sees a transaction onchain, extracts the sequence number and sees what state they are. Depending on what state they are, they need a different payload itself. That’s what we do for the HTLCs and that’s cool because they can collapse down the revocation… into a single thing. We also give the outsourcer a description of what the justice transaction looks like. This is the transaction that pulls away from the cheating party to me or whoever else. We use BIP 69. This allows us to have deterministic ordering of the inputs and outputs. HTLCs require a new signature for every single HTLC that’s there. We do this thing with the txid. What we do is we have the full txid of the state which we assume they can’t predict because there’s randomness in there. We encrypt the blob with the txid. Now when the… takes the id on the chain, they can see if it actually decrypts. If it doesn’t then nothing happens. If it does then they can do that… There are some open questions there as far as how you do authentication. You want to do authentication in a sybil resistant manner because you don’t want someone connecting to the outsourcer and sending them a bunch of garbage, like a gigabyte of random data. Instead we want to make it somewhat costly and that can be a ring signature over some channels, it can be like a bond, it can be a bunch of other things. There are also some compensation questions like do you pay per state? Do you pay once they actually do the action? Is there a subscription model? These are questions that we’re working on and will be answered.

# Scaling Outsourcing - Lighter Outsourcers

For outsourcers, this is a really cool part. So every single outsourcer before it, they had to store either this encrypted blob or the prior state for every single state. If you did a million states, it can get pretty large. If they have thousand clients that have a million states each, it gets prohibitive even with smaller values for signatures and pubkeys and things like that. What we do here is we have this single tuple. Once again, it sees the commitment, the state number, the signature and then r is the randomness we’re using in the commitment. We’re going to replace this every single time. Now the outsourcer has 200 bytes of constant storage on the client and that’s all we’ll replace. The other cool thing about this is that before with shachain/elkrem, the old point, you had to give the outsourcer every single state continuously. The reason for this is because if you ever skipped a state, it wouldn’t be able to collapse the tree down. The way the tree works is that once you have the leaves you can collapse that down to the parent. If I don’t give you one of the leaves, you can’t collapse it down. With this one, I can skip any state I want to. I can give you every tenth state and I can live with that little particular risk. The new scheme itself, we’re going to use that encrypted blog approach. We’re going to add the revocation type which tells the outsourcer how to act on the channel itself. We’re going to give them the commitment, the revocation info, how they do the dispatching. This is pretty cool because now I have constant state but it is a little bit different because if I want to enforce the way the outsourcer can sweep the funds, I need to give them the signatures again. As of right now, I just say “Hey outsourcer. You did your job. You get all of Bob’s money.” It is kind of like the scorched earth approach before but maybe I want to get some money and we want to have this deal on compensation. We can fix that as well.

Did I skip a slide? Maybe I skipped a slide, I think I deleted one. That’s too bad. What I was going to talk about there is that you can basically use covenants on this approach as well to eliminate the sig completely. So you only have that one constant sized signature. The reason you can do this is because the covenant will basically say “Hey. I’m going to sign Bob’s key and Bob can sweep this output.” When Bob sweeps the output, he has to in a particular way that pays me and pays him. This is a cool thing because I can give him this public key and this tuple and he can only spend it in that one particular way. The other cool part about this is that I can add another clause that says that if Bob doesn’t act within three fourths of the way to the timelock expiration, anyone can sweep this. If you’re willing to sacrifice a bit of privacy you don’t need to worry about breaches at all because anyone can sweep the money. If anyone ever tries a breach, the miners are going to do so immediately.

# Scaling Outsourcing - Outsourcer Incentivization

Now we’re talking about outsourcer incentivization. Ideally the outsourcers are compensated because we want people to have a good degree of security themselves. We want them to feel that everything is safe and that if someone tries some infidelity, they’ll be punished. So the question is do you do per-state or do you do on retribution bonus. With a retribution bonus, maybe they say “I get 10% of the value of Bob’s funds”. They have to stick to this because the signature I give them covers this 10%. If they ever take more than that it is an invalid signature. If anything is violated, everything else will fail. Ideally breaches are unlikely so this won’t happen very often. Maybe there are like three breaches a year and everyone freaks out but then everything is ok afterwards. So maybe instead we should do a pay-per state model. This would basically entail me paying the outsourcer for every single state that’s backed up. Periodically, they can remove this but the cool thing about this new channel design is that the state is constant. They are just replacing something. It is not like they are allocating more state per client in terms of the history. There is a cool idea that’s floating around the mailing list from this guy Anton. What he does is that each outsourcer has a specific e-cash token so I pay them either over lightning, maybe it is ZKCP, maybe I give them cash or whatever but I have these tokens now. I unblind the tokens after I receive them. The cool thing about this is that now I can use the tokens but they aren’t linked to my initial purchase itself. Every single time I send a new state, I have to pay the outsourcer, giving him Bob-token and he’s fine with that because he got compensated. We can do both, the best of both worlds. It just depends on how frequently we’re going to see breaches in future and how a market is going to develop around the compensation.

# Scaling Outsourcing - Outsourcer Static Backups

The one last thing we can do to extend outsourcers is that some people talk about how to do backups in lightning itself. There is a little more of a challenge because the wallets are now more stateful. In the case of a regular onchain wallet, you basically just have your BIP 32 seed and if you have that seed you can scan the chain and you can get everything else back. With this you have some additional data, the data is basically the channel which includes some of the parameters of the channel: the pubkeys used, who did I open it with, the size of it and everything else. That’s the static state and there’s the dynamic state which you backup every time with the outsourcer themselves. What if we can ask the outsourcer to store this very small payload? It is going to be like a few hundred bytes at most. We can use this exact same scheme and we can reuse our existing sybil resistant authentication scheme to allow the user to retrieve the blob from the outsourcer. This is cool because now I can lose all my data as long as I have my original seed, I can reconstruct this data deterministically, authenticate with the outsourcer, get my static backup and then join the network itself. There is a risk that that backup is out of date. We have a measure in the protocol that when two clients reconnect to each other, they try to synchronize the channel state and if that channel state is off both participants know. During that process they prove to me that they’re on a particular state and the value they give to me allows me to sweep my transaction onchain. I think this covers the backup scenario pretty well. It depends on the emergence of the people running these watchtowers or outsourcers itself. Then there’s the question of who watches the watchers? Who makes sure they’re storing the data itself? What you can do is have a proof-of-retrievability protocol over what their storage is itself. So me as the client I say “Hey watcher. Can you watch this guy to make sure… pull repeatedly… to ensure he’s storing my data” Then if he doesn’t provide it to me because it is a random challenge, I stop using them. At that point they’re storing the data but they’re not doing what I told them to so they don’t get compensated anymore. 

# On-Chain Succinctness - 2-Stage HTLCs

So the final thing now - second stage HTLCs. So the way HTLCs work now, they actually aren’t shoved into the commitment transaction because we realized that there were issues as far as the coupling of the timelocks. What happened before was that if you wanted to have a longer CSV value which is kind of like your security parameter in terms of you being able to act on something in a timely manner, you had to have a longer timelock as well. That was a pretty bad thing because as you have longer routes, you would have timelocks of like 30,40,50 days, a very long time. So what we decided to do was to separate that. Now the HTLCs, once you claim it is a two step process. Now there’s the HTLC transaction, there’s another one and once you broadcast that one you’re waiting for the claim process, that’s a CSV delay value. Afterwards you can claim it. This is cool because now the CSV value and the CLTV value are in two different scripts and they are fully decoupled from each other. There are some cons to this. The con is that now we have a new transaction for every single HTLC. Every time we update a state, if I have 1000 HTLCs I give you 1000 signatures and you verify those 1000 signatures for me. Additionally we have to store the signatures for every single one of these HTLCs if we want to eventually be able to handle breaches in future. Also the outsourcer needs to handle these HTLCs themselves because we now have covenants. What we’re doing here is we’re basically making an offchain covenant with 2-of-2 multisig. We can say that if we have real covenants we don’t need the end anymore. The goal of the covenants was to force them to wait the CSV delay when they were going to claim the output. But with this they can only spend the output if the output they created in this transaction actually has a CSV delay clause. So we’re basically fully enforcing in the script itself which is pretty nice. A stop gap if covenants don’t ever get into Bitcoin itself is we can do this thing with SIGHASH flags to allow you to coalesce these transactions together. Right now if I have five outgoing HTLCs in my transaction, I have to timelock those onchain. There are actually five different transactions which is not really ideal. What we can do instead is allow you to coalesce them into a single transaction by using more liberal SIGHASH flags. This is cool because you have that signature transaction get confirmed, four blocks afterwards you can tweak that into your own outputs and that works out just fine.

# On-Chain Succinctness - Multi-Party Channels

So now we’re talking about multi-party channels. There is some cool work by some people at ETH doing some work on multi-party channels. It is a different flavor, multi-party channels because there are some issues around are you going to make the hierarchy flat? Who pays for the punishment? If I want to update my own state do I have to wait for everyone else to be online? So what they did instead was make this hierarchical model of the channels themselves. There’s a few different concepts. There’s something called the hook. The hook is the master multisig. What the hook is is that if there are five of us, that’s a 5-of-5 multisig in the naive case. Assuming signature aggregation, we can actually aggregate that down to a single signature so it looks like a regular channel. Even though we’re making this 200 by 200 party channel, this saves a lot of space on the chain itself. We can make that more succinct by using signature aggregation to compress that down. Once you have that hook which creates the giant multisig, you have this thing called allocations. What the allocations do is  decide on the distribution of funds inside each channel. If you look at the diagram it is basically just a series of 2-of-2 channels and those 2-of-2 channels go up to an allocation eventually, go up to a hook and you can keep doing this over and over again. The cool takeaway here is that they realize you can embed a channel inside of another channel. Even go a channel inside of that. A channel inside of that and so on. Every single time you make a new channel it can be completely different rules. I could have a bidirectional channel on one side and then go on another side and have a multiparty channel and all these other things. No one from the outside knows and we can do all of this stuff collaboratively offchain without doing any onchain transactions themselves. This is a pretty cool thing but there are some issues as far as the state blowup onchain with this. One of the reasons they didn’t use key based revocations is because you have this weird state blowup. At this point you have asymmetric state. When you have asymmetric state you have this blowup as far as it scales with the number of participants itself and that can get really prohibitive. What they did in the paper was that they used the invalidation tree again so basically any time you want to update the hook…. and you would need to update the hook if you want to do different allocations. So if we decided “I don’t want to be in a channel with Bob, I want to be in a channel with Carol” we would need to update that hook eventually depending on the distribution of the channel itself. That would require this new invalidation mechanism which we talked about in the past. One reason that right now they use that tree based validation, using the invalidation tree… an undesirable part about that is that that can get very large. What can happen is that as you update the hook more and more, you have a tree with a bigger depth. Now I need to traverse a tree of size h to even get to the hook. Below that there are even more transactions. At this point you could have hundreds of transactions onchain just to force close. What we can do is reuse the signed commitments. We can reuse the signed sequence numbers for the hook update. This makes it even more scalable. Now everyone has constant sized state independent of the number of the participants in the channel itself assuming I’m keeping track of my own individual transactions. Also this has the same benefits as far as outsourcing. If we’re using signature aggregation everything is very tiny and that works out pretty well.

# On-Chain Succinctness - Fee Control

Final thing, fee control. One issue that we realize is that right now it is not super easy to modify the fees on the channel itself once it has been broadcast. One issue is that, let’s say my peer was offline for a while and I wanted to force close the channel. The way it works currently is that that fee on the channel itself is actually locked in ahead of time. Let’s say I was offline for two days and fees were to do some really crazy stuff, it turns out that my transaction has insufficient fees to get confirmed in a timely manner. At this point, I can’t do anything. I could use child pays for parent where you basically create a bigger fee package that has a higher fee rate. I can’t do that because the output is timelocked. Because the output is timelocked, I need a way for the timelock to expire in order to sweep it. In order to sweep it, I need it to confirm. So you’re in a very weird position. One idea here is to have a reserve hook. What this means right now in the way the design works is in order to avoid a costless attack, we ensure each participant has a certain percentage of money inside the channel at all times. This avoids them saying I’m on state 10 and I have zero money, I’m going to go back to the prior state. What the idea is here is that creating an anyone-can-spend reserve hook output means anyone can use to anchor the transaction now. This means that even if I mess up guessing my fees initially, I can still use this to eventually confirm my transaction. Going a little bit further, we can actually eliminate this fee guess game. Right now in the protocol, we have something called update_fee and the initiator basically says “Hey. I want to update my fee because the fee went down or the fee went up.” The issue with that is you have to guess ahead of time. If there are very rapid changes in the fee structure or the fee climate on the chain at that particular point, it can get kind of difficult. The other thing is that it is hard to guess, fee estimation is fundamentally a hard problem. The solution here is don’t apply fees to the commitment transaction at all. The commitment transaction has zero fees meaning you need to add a new input to get it confirmed. What we’re going to do here is again use some more liberal SIGHASH flags to allow you to add an input and an output to the commitment transaction. This means that only after I’ve broadcast do I need to decide what the fees are. One issue that this brings up is that whenever I add this input, the txid of the transaction actually changes. If we had any dependent transactions from that, those all become invalidated. So what we do Is we sign that with NOINPUT. So NOINPUT was from a while back, before SegWit. What NOINPUT does is right now when you sign an input, it basically signs the txid of the transaction that made the input and the position of it. We’re saying don’t sign that, only sign the script. What this means is that I can have a dependent transaction and that is still valid even if the parent transaction changes because the public key script is still the same. So using this is pretty cool because we don’t have to worry about fees at all. We can regulate our fees completely and this gives you a lot more control in terms of confirmation time of your output or anything like that.

# Q&A

Q - So to sum it up, the upgrades for Bitcoin script are covenants, SIGHASH_NOINPUT and CHECKSIGFROMSTACK?

A - Yes. With CHECKSIGFROMSTACK and NOINPUT we can do a lot. Covenants is the holy grail and I didn’t even get into what you could do here. I think covenants, there’s some controversy around that. I think it is more likely for NOINPUT and CHECKSIGFROMSTACK to get in and those are very minor additions. …CHECKSIGFROMSTACK in 2015 and super small. Same thing for NOINPUT, it is just a new SIGHASH flag. They are very minor additions to Bitcoin yet the possibilities blow up. Now all of a sudden you have this signed data. You may think it is not that powerful, it is extremely powerful in practice because people have shown in the past that if you have CHECKSIGFROMSTACK and OP_CAT you can actually do covenants itself. Maybe we’ll accidentally enable them but I think people are going to be pretty vigilant and catch that. Yeah those are the two updates.

Q - There have been a bunch of discussions on how large centralized network hubs can potentially lead to some sort of censorship. I guess my question is have you done any research on how to keep the lightning network decentralized?

A - There’s zero barrier to entry at any given point. Anyone with some money, a computer and some Bitcoin can join the network itself. There are also very small switching costs itself. I think the actual requirements to have that much capital in a massive hub are going to be pretty prohibitive. At the end of the day, you can route around them. There’s not really censorship because depending on the routing protocol, we basically use onion routing. So they don’t really know, maybe there are some heuristics they can use, but they don’t know exactly who I’m sending this money to. Even if they cancel my transaction, I’ll route around them. 

Q - I was just curious if there was a limit to the number of state channels you could have at one point. If not, would the creation of unlimited state channels be an attack vector?

A - Sure. The upper bound is the number of the outputs you can have in the UTXO set. But if you’re looking at this, we had one output and we made 20 channels. At that point you can keep nesting this stuff. It is effectively unbounded. You need to get into the chain and if you’re trying to sybil attack, you’re paying money to get into the chain. There’s some cost there.

Q - In the beginning on one of your first slides you said there was an assumption that attackers were not colluding with miners. Could you talk a little bit about what the potential is if they were to collude with miners?

A - Let’s say there was one miner and I had this massive channel, I was doing gambling or whatever with Charlie and then Charlie and the miner collude. What they can do is they can let Charlie’s transaction get into the chain and don’t mine my transaction at all. That’s the worst case, the base case. You could say that if miners were more decentralized, even if it was just 1% chance that I could get into the chain then that’s good enough for me. The bad absolute where miners basically censor transactions willy nilly then things start to break things down. Especially if they are reorg-ing blocks back and things like that. At that point, we’re doing a bunch of other stuff in Bitcoin and maybe we’ll get around that. There have been some proposals doing this committed transaction format where they would mine a transaction and maybe they know it is valid due to a zero knowledge proof but they don’t exactly know what it is spending. Only later on can it actually be revealed. If you can have some commit and reveal thing for transaction inclusion you can get past that as well.

Q - I’m curious about the… as you upgrade the protocol how do you anticipate that upgrade schedule for the new..  I guess there’s the wallets and there’s also the hubs.

A - The way the protocol is, there is a bunch of extension points in the protocol itself. In the onion routing protocol but also the… service bits. Right now I could make another new transaction or new channel type and it wouldn’t disrupt anything at all. It wouldn’t disrupt anything because we have that same shared Layer 3 which is the end-to-end layer which is the HTLC. So even though we’re speaking different protocols and individual links we can actually have the same HTLC end-to-end and everything is fine on that front. If we want to.. we basically add this new feature bit, so when I’m connecting to you, we exchange feature bits. I check do they know about the new super shiny fancy channel. If they don’t maybe I disconnect and I go to someone else. I foresee it being pretty gradual and even in the future I think there are going to be a bunch of different channel types, there’s going to be a bunch of different routing layers. I think it’s all going to work together because everything is at the edges and that’s all that really matters. That’s a cool design element of the way it turned out.

Q - I especially like this multiparty channel... associated with the… multiple secure computing. Can you elaborate on some of the research work on secure multiparty computation?

A - There are one or two protocols that are enabled by CHECKSIGFROMSTACK. Inside the NPC if you can actually generate signatures and those signatures sign particular states, you can use that for invalidation as well. Multiparty channels have a bunch of cool things because now you can pool liquidity. The five of us can get together and make a node that is auxiliary among us which can pool our money together there. There are a bunch of cool opportunities in terms of games. Imagine you have a North American game server and a South American game server. Each of these one multiparty channel and all the economy is done through that. You can still route through them. You can imagine that commerce is more frequent between North America and South America and when we want to do routing it still works.

