talk title: Expected and Unexpected Developments in Quantum Computing

event: PQcrypto

video: <https://www.youtube.com/watch?v=nJxENYdsB6c>

LLM-generated summary:  Sam Jaques presents a critical assessment of quantum computing progress toward cryptographically relevant scales, emphasizing that breaking RSA-2048 via Shor's algorithm requires ~20 million noisy physical qubits under surface code error correction—a factor of ~200,000 beyond current ~100-qubit devices like Google's superconducting processors—extrapolating to ~2052 under Moore's-law-like qubit doubling every 1.5 years, though earlier full-stack estimates from 2019 may soon be revised downward due to advances in catenation for T-state distillation, Regev's parallelizable factoring algorithm, residue-number-system optimizations, and fast multipliers, each offering at most constant-factor reductions. He critiques hype around non-scalable "logical qubits," dismisses random circuit sampling as practically useless despite engineering feats, highlights surface code's optimality for nearest-neighbor architectures amid skepticism for long-range LDPC codes and unproven topological/cat qubits, and outlines milestones like encoded gates by 2027, surpassing current factoring records by 2032, and quantum advantage over classical factoring by 2049, underscoring exponential scaling needs and the field's immaturity with many known unknowns.

[[!!toc levels=3]]

# Introduction and Motivation

Hello, welcome to the third invited talk of PQ Crypto 2025. We have Sam Jaques from University of Waterloo with "Expected and Unexpected Developments in Quantum Computing."

Hi, thank you. Thank you for the invitation. I'm really happy to be here.

So there's sort of a joke title here. The reason we all have this conference is we want to develop kinds of cryptography that will be safe against quantum attack. And so then this is because we expect quantum computers to somehow break all of the nice cryptography that isn't. And so the question is, how's that project going to actually make these quantum computers?

So what I really want to ask is kind of take this perspective of when are quantum computers going to break RSA 2048? And sort of say, hey, all of this good work that we're doing today is worthwhile and should be deployed.

# Outline

So the outline I have here is kind of an introduction to quantum computers, which is a weird thing to do also at this conference. I think half of you at least will know a lot about quantum computing, but some of you will sort of be thinking on implementations and mathematics.

And then where quantum computers need to go to break RSA 2048 and sort of this business as usual path. So kind of the clearest path that we can see, which is still going to be extremely difficult. And then the things that have happened lately that might kind of disrupt things. So that will sort of the known unknowns on this path.

# Quantum Computers Basics

So a quick introduction to quantum computers. So what is a quantum computer? It's a collection of qubits. So this is some sort of device and it holds quantum data. So it's a zero or a one with these funny ket notation and then linear combinations of the two. So this is where a lot of the quantum magic comes from. So you have these linear combinations. You can have negative coefficients, you can have complex coefficients, and then you can have multiple qubits that entangle together.

So what does a qubit look like? Really, any kind of two-level quantum system can be a qubit. So you can have superconducting qubits, and in a very simplified way, you can think of it as having current flowing in one direction or another. So IBM and Google are pursuing superconducting qubits so you have this giant machine that's what that is. You can have trapped ions so you have an atom kind of in some sort of different state and this is the states of your qubit. You can have photonic qubits so you'd have a photon and it can be in one of two different places so think of two different fiber optic cables and you can even have superpositions of this and this is where the quantum computing comes in. And there's lots of other technologies.

And once you have qubits, so you have quantum data, obviously you want to compute on the quantum data, so you need gates. So this is some sort of process that will modify the data. And I want to emphasize here that unlike a classical computer, we'd usually think of quantum gates as being some sort of process rather than a device. So if you look at Google's machine, there's nothing you can point to there and say, that's a quantum gate. If you read a paper that says this needs two billion gates, you don't need two billion components, you're just doing something two billion times to the qubits themselves. So the quantum circuit diagrams, don't think of actually printing them onto a chip, like you might think of a classical circuit diagram, think of them as sort of a series of instructions.

# Noise in Quantum Computers

And the other big thing that is really important for quantum computers is noise. And this is just anything that's going to modify your quantum state that you didn't want to happen. And the reason this is a much bigger deal for quantum computers and classical computers is these are fundamentally just more sensitive systems. So you kind of need this quantum space. If you have just a little change to the state, this can accumulate over time and create big problems for your algorithm. And classical computers basically don't have to deal with that. If you're doing digital classical computation, if you have some part of your wire get a little bit more or a little bit less voltage, that kind of, you know, gets kind of reset back. So if you're going to be definitely some hardware people who will disagree with me on the importance of these tiny perturbations and faults in a classical computer but mostly we get to ignore this.

The other thing is qubits have an entire other dimension of error. So, as demonstrated in this meme here, you can have a zero flip to a one, just like you can classically. You can also have this phase flip error. And there's no classical analog. Now, arguably on its own it's not actually so bad, but correcting both of them is very difficult. Any linear combination of them so in some sense there's an infinite number of possible errors your quantum computer can have so it's restricted to two dimensions but this extra dimension makes certain things fundamentally more difficult so think about a classical hard drive um where you're representing data with magnetization if one of your atoms decides to flip its magnetization the other atoms basically bully it back to where it was before you can't have a similar quantum system so it's just even even kind of a provable result in certain dimensions so thanks to this noise becomes a really big problem that we need to think about and pay attention to when we're looking at where quantum computers are.

# Current State: Error Rates vs. Qubit Counts

So what I like to do is draw this chart of error rate and number of qubits so I think these are the two most important pieces of data to summarize quantum computing. Naturally, I've kind of ignored a lot of stuff to compress this down to two dimensions. So for example, these ions at the top have certain connectivity that the Google machines and the IBM machines don't have. So I've completely ignored this to put them on one axis. Error rate itself is not one-dimensional. You have lots of different kinds of errors. And putting Google's point there. Someone at Google complains that I'm paying attention to the, I think, the measurement error rate, which is not actually as important for what they want to do. But this is not to say, you know, Google is better than IonQ. Basically, I just want to say, roughly, where are we today? So I'm happy if this is a little bit imprecise, just to give a rough picture, because the question is, where do we need to go? So, and Shor's algorithm is way up here. So it kind of doesn't matter if I was a little imprecise at the bottom, there's this massive nine orders of magnitude gap in error rate and roughly two orders of magnitude gap in qubit count that we need to cross.

# Error Correction and Surface Codes

So how do we possibly do this? How are we possibly going to you know this is the progress we made so far. How are we going to cross this giant barrier? And the answer is we won't. People don't really expect us to build qubits that good. And so the other big thing in the progress of quantum computing is error correcting codes. So the idea is just like a classical error correcting code, you have some redundancy in your qubits. So rather than having one very good qubit, you would make a whole lot of them you'd somehow encode your state in the joint state of all of these physical qubits.

So, a physical qubit, a little terminology that gets thrown around. These are the things like today's qubits. So, the superconductors, the trapped ions, the fiber optic cables. And then the logical qubit is this abstraction. So, I think the talk yesterday about, you know, reducing the number of qubits to solve LWE, this is kind of logical qubits. The qubits need to be high quality for this algorithm to work. So then each of those logical qubits would, in practice, actually translate into a number of physical qubits.

So why do we think in this way? Basically, it seems like one qubit with error rates that are a billion times better than today is going to be way harder than making a thousand of them and ten times better. So once we can make good enough qubits, it seems like scaling them should be easier. Building a big computer should be easy to do.

So how do we do the error correction? What does it actually look like? The leading candidate right now is something called a surface code. And it looks like this. So you have this grid of qubits, and they're each connected to their nearest neighbors. And what this is doing, kind of it suppresses errors exponentially in the grid width. So every time I increase the width of this, this little square patch here would be one logical qubit, and each of the dots is a physical qubit. If I add one more row and column of physical qubits, I get a multiplicative reduction in the error rate.

How does it do the error correction? You do this circuit at the bottom here, where you're just kind of applying some sort of quantum gate and doing some measurement probably thousands to millions of times per second.

Why are these the leading candidate? So the error detection itself is fast and simple. One of the things is because the gates themselves are so noisy, you can't do that much. You can't do a very complicated code because then you risk that the error detection and the error correction will actually introduce more noise than it is able to correct. So you need that process to be simple. Surface code does this. The physical connectivity is simple. You just have this 2D grid. So this is something we actually, superconducting qubits, have this structure. The architecture is there. So we don't need any kind of unusual assumptions. There isn't another engineering challenge to solve to build that.

Another big one that I'll get back to later is that we actually know how to compute on the encoded quantum data. And if you're used to classical error correcting codes, this might seem a little weird because, say, you know, my phone has some data that it sends to the Wi-Fi. It encodes it. It transmits it. The router decodes it. Because both the router and the phone have a CPU that runs at a high enough quality that it doesn't need to do error correction for day-to-day computations. Not true for a quantum computer. There's no part of it that can hold the data well. So a superconducting qubit gets about 70 microseconds of holding onto its state before it's gone. So you really need to be doing error correction all the time. So if you want to do your computation, you need some way to do computations on the encoded data without ever actually decoding it. And this turns out to be challenging, but we have, and this is also point number five, we have kind of at least a decade of work on solving all of these little issues about the surface code.

And the ratio of physical to logical qubits is not great, but it's probably good enough. So kind of at the scale of Shor's algorithm, up to an order of magnitude, it's about a thousand physical qubits for every logical qubit.

# Recent Surface Code Demonstrations

So where are we today with surface codes? Something very exciting last fall was kind of this big experimental demonstration out of Google. They had a lot of interesting things in this chart. So, you can see all of these curved lines. So, what are they? The red curved line was a small distance surface code, and you can see that the error correction cycle, which is basically time, is going from left to right, and then the error probability is going from bottom to top. So, as time goes on, you have more of a chance of an error which makes sense you know. That error probability kind of has to be cumulative. But you can see in these blue lines as they increase the distance of the surface code that error probability goes down. So they're getting better, basically the errors are being suppressed as they're supposed to be.

So looking at this chart, theoretically if they took their technology now and just made it even bigger, they just scaled it up, they could get, they could suppress the errors even more by using just a bigger code. And another exciting thing about this result is the dotted green and yellow lines, those are the actual physical qubits, so now the logical qubit is outperforming the physical qubits. And other things that they're doing that are exciting, they were decoding in real time, so they're actually keeping up with a massive amount of classical co-processing necessary to correct errors.

So I want to compare this briefly just to contextualize this with other quantum news with the Zuchongzhi. So this is a superconducting chip from China that has the same number of qubits, and in their abstract they compare it and they say this is doing much better than Google's chip with this 32-cycle random circuit sampling.

And this is a talk where kind of deliberately I want to be very critical of basically every advance in quantum computing because I think broadly it's an overhyped field. This is also a talk where I'm basically making an enemy of everyone because I'm going to say, well, I don't know that this is actually that great.

So when I look at this, you've got kind of some of the qubits that are, they've excluded because I think they just weren't working well enough. When I try to put this on this chart, it comes out way below. Basically the error rates are much higher. And so I think even if they're doing better random circuit sampling, they're not able to do the kind of surface code demonstrations that Google was able to do. And I think the surface code is a much more interesting thing to do because it actually gets us eventually to Shor's algorithm.

So random circuit sampling, it's an impressive thing to do. Certainly, I have no idea how to build a quantum chip that could do what their chip does. But at the end of the day, it's not doing anything useful. It's not doing anything that's going to scale up and solve some sort of problem. Or in the case of cryptography, create some problems. But there we are. So this is why I like to pay attention to the error rate in addition to the number of qubits, because it kind of highlights issues like this.

# Business-as-Usual Path to Breaking RSA 2048

So given this this leads me to my business as usual path. So the steps to break RSA you make the superconducting qubits a little bit better you grow them exponentially because you need to build enough of them, roughly 20 million to factor. So Google's chip is about 105, so we're about 200,000 times away from a quantum computer this large.

So I pointed out all these engineering challenges. So you have this massive increase in the qubit counts. The error data throughput that you actually need to deal with just to maintain your qubits, like the classical coprocessing, is something like 100 plus gigabytes per second of just measurement data, of data on the errors of this machine. So you've got this enormously powerful classical coprocessor just to maintain your quantum computer. Dealing with that in real time is going to be a really big issue.

Where do you put all these qubits? You need to keep them very, very cool with superconductors. So do you build a giant, almost stadium-sized fridge, or do you connect them? Other issues, you have these cosmic rays, so basically your chip can get struck by a ray, and then the qubits around there are just kind of out of commission for a time. And there are ways to deal with this, but it's challenging.

So I point this all out to basically say business as usual is a misnomer. This is the kind of path where whoever is leading the pack here in this technology is basically getting a breakthrough nature paper every year just to maintain this path. But we have sort of the outline. This is where things would have to go. So I'll assume that we kind of solve these challenges as they come up.

So here I put the resource requirements using surface codes on the right. So the error rates we can now drop way down. We don't actually need that much better error rates, but of course the number of qubits has now jumped up. So we need to jump up a factor of 200,000 times.

So how do we predict this? Well, if we just take Google and we just take two data points and extrapolate, then they had a 45% increase in qubit counts over two years. And so let's say we grow exponentially at that rate. We break RSA 2048 in 2088, assuming some sort of stalling out of the physical error rate. But you can see that this isn't gonna matter that much, even if they bumping up their physical error rate it not making that much of a difference. The lines here for the resource requirements actually are very steep. So once you using a surface code the trade is very much in favor of just building more qubits. You don't get that much of an improvement by making them too much better.

This is quite a pessimistic picture. This is actually more of the yes answer to my joke question on the first slide, but I don't think this is a realistic estimate. So Google is doing a lot of other things. And I think right now it's really hard to draw trends from quantum computing. I don't think there has been enough consistent progress in any way in quantum computers to look at it and say in the same way that classically you have Moore's law and it's very tight. Like if you look at the graphs of this, they're a very clear correlation. No such clear trend for quantum computing.

So I'm just going to kind of guess here. I'll say, well, what if quantum computers happen to grow like Moore's law and I decide to interpret Moore's law as saying that the qubit count will double every one and a half years. So we get something like 2052 as roughly the time to break RSA 2048. So this is kind of the business as usual path. And again, I say it's not really usual because it's suddenly we have hit this exponential growth.

# Predicted Timeline

So what does this look like as we as we live through this so in 2024 we had our basically the first surface code logical qubit more or less. 2027 we have enough of them to start demonstrating some gates on the logical qubits so kind of the very first computations on encoded quantum data this will be CNOTs and T gates so that'll be an exciting result to look forward to in the next few years.

By 2032, we'll start to have more logical qubits than we currently have physical qubits. So notice that actually we kind of have to move backwards. Already at distance 7, it's about a 100 to 1 ratio of physical to logical qubits. So if we want more logical qubits than we have physical qubits, we have to improve by a factor of about 100 from where we are today. And this is sort of an interesting thing to think about, that once we've encoded things into the surface code, we've sort of gone back to having very few qubits compared to where we are today.

And interestingly, this is the point where quantum computers finally break their current factoring records. So, there's a bit of a cottage industry of pretending to factor large numbers, where you find some way to take your factoring algorithm, take Shor's algorithm and compile it down in some secret way, or choose some very special number to factor, where it's extremely easy for the quantum computer. In the most extreme case, it's just flipping a bit. Like you could factor by flipping a coin. But there's less disingenuous ways that are still not quite there.

If you actually want to run Shor's algorithm kind of the way it's supposed to be run, today's quantum computers can get maybe 21, maybe 35 if you really stretch it a little. And I just don't expect they'll go any higher because, you know, Shor's algorithm has kind of a cubic number of gates. We're just not getting the error rates we would need to run that without any kind of error correction. So by the time we have enough error correction to actually break our factoring records, it's already 2032. I'm not really sure what we'll see in the middle. I can't think of any good milestones, so there's just the halfway point.

2044, this is, I'm not really a quantum chemist, so this is my least confident claim here. This is taking a number from one of Google's quantum chemistry papers and putting it on here. This is maybe when we would see some, maybe some commercial applications of error-corrected quantum computing.

This one I think is quite interesting. 2049 is when quantum computers actually start to have a quantum advantage in factoring. So we're really good at classical factoring. So to beat today we would need to factor 830-bit integers, and if you plug in that number into the resource estimators it says you need a number of resources that's going to be roughly here, again if we're scaling exponentially. And then three years later, RSA 2048 breaks. And this also assumes that we haven't made any classical progress, that we're just sort of saying, well, let's not make any more number field sieve records, let's keep it at 829. So it's really this sharp transition from quantum computers are still not as useful for factoring as classical computers, and then all of a sudden RSA is broken. So this is kind of what the fact that Shor's algorithm is a polynomial time algorithm but to get there we probably need exponential scaling. This is what it would look like in practice.

# Sorting Through Hype: Physical vs. Logical Qubits

I want to point something out here. So in my abstract, I promised something that I was unable to deliver, which was to kind of give you the ways that I look at quantum computing news to kind of sort through the hype. And there's so many different ways to hype quantum computers, I actually don't think I can effectively say, here's a set of tools to look for.

One thing I will point out that I've kind of seen is, I think people have started to, more and more people are paying attention to the difference of physical versus logical qubits. And I think this is really great because it's essential to understanding quantum computing. The problem is now because so many people are saying, ah, but what's the logical qubit count? Now you express logical qubits in the abstract and in the press releases.

So here is this paper. And again, I'm making enemies. I just sort of picked this one randomly by, I think, searching logical qubits in arXiv. So I'm not picking on this one in particular. It says they have 28 logical qubits and I just said the best with one logical qubit. So what's with this? Why is there this discrepancy? This is not a surface code. So this is not going to scale well. We're not going to be able to compute well with this. This is a nice demonstration. But it's kind of misleading to think about it in the same terms as Google's results.

So this is not the first time I'm going to pick on Microsoft in this talk, and I apologize for this one. Everyone does this. It's not like Microsoft is doing something wrong here. This is the correct terminology. Logical qubit just means a qubit encoded in a code. So I'm just sort of saying this as a watch out when you're looking at quantum computing news. If you see logical qubit, it might not mean the kind of logical qubit that we're going need for factoring.

# Potential Accelerators: Better Hardware

So looking at this timeline, PSI actually predicts, I think this is their conservative estimate was, I think, 17 years. And they were saying 2041. So why this 11 year gap? So I had this number of 2052 and they said 2041. Why are they being so much more optimistic about quantum computing than I am? And I think it because while there also challenges that might pop up that make this more difficult there other things that might make this whole thing happen faster.

So I want to focus on four different ways that this timeline could change. So we could have better hardware better codes better algorithms and better implementations.

So, what could the better hardware be? So, looking at Google and extrapolating from there, that's superconducting chips. And they're kind of leading right now, at least in my opinion, but I think someone from NIST gave a talk I saw a while back and mentioned, well, what's the transistor versus the vacuum tube of quantum computing? Are we still working with vacuum tubes and something's going to come along that will be more the transistor, or is the transistor really, you know, an error-corrected superconducting chip? And there's a bunch of different technologies. They all have very cool, sciency-looking pictures, so I liked finding these to put this slide together, but ultimately it's still early days. I don't think anyone could confidently say which one is the best. Lots of smart people have different opinions on this.

So I want to talk about two, just to give some flavor, partly because they were in the news recently. So topological qubits. So this is the idea where your qubit is this so-called Majorana quasi-particle. And a quasi-particle is you have a collection of particles and they're acting in some way that looks like another particle, even though it's not really. So this would be like waves on water. Before this was a PDF, this was actually an animated GIF of a slinky. You can see this little pulse going through the slinky. But I think you can imagine that for yourself just as well. This is sort of an intuition for what's going on here.

Because there's many real particles, again, really, I don't have a clear understanding of the physics of a Majorana quasiparticle and I'm trying to simplify it so this is not a great explanation but roughly speaking you've got a lot of real particles so it's going to be harder to cause an error you sort of have to do a lot to push it to another state and it ends up making these little these string diagrams where you're braiding these Majorana particles around each other this is how you do the computation.

The idea here is you would build some sort of physical qubit that would jump ahead in its physical error rate. So, did Microsoft make topological qubits? You may have seen this in the news recently And it a bit of a contentious issue. So when I was looking at the Wikipedia page for Majorana one their chip sort of a concerning Wikipedia note here non source needed for show some signals of hosting boundary Majorana zero modes. Nature is publishing all these opinions of other physicists who are skeptical of this. You can read the peer reviews, which is an interesting read, actually.

So, the peer review files for this publication are completely open. And this one's quite interesting. It says, you know, these are very interesting experiments, certainly relevant for condensed matter community. They say what I do not like is the way the article is written which sometimes subtly and sometimes more crudely uses a language and wording that at all times leads the reader to think that we are dealing with a measurement that demonstrates parity in a topological qubit based on Majorana states.

So this is the kind of pushback that Microsoft is getting for this. I don't know. I'm definitely not – I don't know enough about physics to tell you whether or not they actually have this right now. The other thing is because the peer reviewers, as you can see, were quite critical, it took them a year from submission to actually getting this published and they've claimed that they've made more progress since they submitted.

So for now I'm going to say, let's assume Microsoft made a topological qubit. Let's give them the benefit of the doubt for now and say where does this fit into our progress. So even though it does improve on the error rates, it would be a much better physical qubit, it's not perfect. I've heard some people say, you know, these topological qubits are immune to noise and this is clearly wrong. The grenade test, which is I throw a grenade in your quantum computer, this clearly causes an error in the quantum state so you can't be immune to noise but it could jump up so in another paper that Microsoft put out a couple years ago they were trying to do these resource estimations and they estimated between 10 to the minus 4 and 10 to the minus 6 for a Majorana qubit so if we take their estimate and we have similar similarly fast doubling and we assume they have one qubit today it's actually about the same time that they get to breaking RSA 2048 and kind of the problem that they would run into is that the resource again surface codes really once you're in a surface code you're not getting that much benefit from lower error rates so even though they're at this huge advantage in terms of error rate, it's only translating to maybe a factor of 10, a factor of 100 in terms of qubit count.

So I'm being highly critical here, but as far as the project of building a massive cryptographically relevant quantum computer, I think it's a good thing that Microsoft is doing something like this just because it's nice to have a lot of diversity in approaches. We don't want to put all the eggs into superconductors and find out that there's some sort of big obstacle. They could pull ahead here.

A similar thing is cat qubits. So Amazon recently put out this nature paper demonstrating these cat qubits. The idea here is that you have something that physically suppresses one of these two dimensions of error. So if you remember me saying that there were two dimensions of error in quantum computing, the cat qubit makes the bit flip errors exponentially harder, but the phase flip errors linearly easier. So it's kind of, you're better off overall, generally, and what this lets you do is use a biased surface code. And so then your surface code overhead actually can be significantly reduced because basically it's only trying to correct one type of error.

So using a resource estimate from 2023 they said that with cat qubits maybe you could factor RSA 2048 kind of there on this chart and Amazon is going to be down here so it really seems like maybe only 15 doublings compared to 18 for superconductors so arguably this seems like a better route but i think there's going to be a lot of challenges from scaling up what they currently have now of about eight of these qubits to the sizes where you could actually maybe feel confident that they'll be able to scale this up exponentially.

So my opinions on both of these is kind of it's a tortoise and a hare. Google is taking this slow and steady route. And again, I say it's not that steady. It's going to be extremely challenging. But Microsoft and Amazon are picking some strategies that might just leap ahead. So it's a little riskier. Maybe Microsoft will just be unable to make these topological qubits and then that's going to really put delays in their progress.

So I think one interesting thing to look for is is there going to be a point where a bunch of these physicists go okay Microsoft has actually done it now They have genuinely made a topological qubit If you see that that'll be quite an interesting thing. The other interesting thing to look for is, would there be surface code error correction using some other technology that isn't superconductors? And that one result with the Rydberg atoms, but I think they had some issues with, they weren't doing as much of the actual live error correction you can do with superconductors.

# Potential Accelerators: Better Codes

So we could do better hardware, but we could also think about better codes. So with roughly 20-ish years of hindsight, we can now say that the surface code is quite simple. You take two repetition codes, you take a very special kind of cohomological product, and now it's your surface code. So arguably, it's nearly the simplest code you can construct. So now that we have the hindsight to say it was an easy thing and should have been obvious. But actually it's not very good. So it's asymptotic rate is zero. Which means that if I want to encode more and more data, the physical to logical qubit overhead gets worse and worse.

And so we could think, isn't there something better? We have classical codes that have constant rates. Could we not make a quantum code? And the answer is yes. So in 2021, we're kind of this big breakthrough result on constant rate LDPC codes. So LDPC is a low density parity check. What this means is that if you want to do the error correction process, you measure these parities. And since it's low density, that circuit is relatively simple. So that means you're not introducing more errors than you're correcting, as long as your physical qubits are sufficiently good. So that is kind of a minimum practical necessity.

And so the physical to logical qubit ratio could maybe be brought down for practical constructions from 880 to 1 with a surface code to something like 14 to 1. This is a massive improvement in overhead. So we take that overhead and we throw it in this chart. Then with the new codes, we maybe jump way down to here. So maybe only 2039 with this Moore's Law scaling.

So what's the catch? What's wrong with these codes? And I can see two pretty big unsolved issues. The first is the long range interactions between qubits and the second is that we don't know yet how to compute with them.

With the surface code it nearest neighbor connectivity This fits the architectures that we have for superconductors right now They have been demonstrated It fits nicely. With these new LDPC codes, we don't know. They need a bunch of long-range connections. So you just can't do them with a superconductor. The architecture does not allow it. And in fact, there was a result basically the same, very, very close to the publication of these asymptotically good codes saying, in fact, they're pretty much optimal. If you have a high rate code, you need these long range interactions, sorry. And this is actually a necessity. There's kind of this trade-off. And the surface code, if you look at their formulas, is pretty much right on the bounds that they give. So the surface code is more or less optimal for codes that are nearest neighbor, that can be implemented with nearest neighbor.

And so the question is, can we build a device that would allow these interactions so that we could actually implement this code? And is there such a technology? Well, ion traps, so Quantinuum says our systems have unparalleled fidelity combined with all to all connectivity. So an ion trapper will tell you definitely this is something you can do with ion traps. You can address any pair of qubits you want. And as far as I know, this is completely true for the ion traps that we have today. I remain skeptical about how well this would work once we have a trap with hundreds of ions in it. And I have no good theoretical reason to say this or no provable reason. It just feels like it would be a challenge to me to kind of have this quadratic scaling in gates that I want to be able to address, be able to do that with high enough fidelity and do enough of them simultaneously to actually make the code work in an ion trap. So again, this is one of my less confident claims here, but I am skeptical about this.

Another thing is, and I think this is probably less of a big deal, this one I feel is more likely to be solved, but how do we compute with these codes? So I said we have to be able to compute on the encoded quantum data because we can just never decode it. It would decohere way too fast. And the surface code, we know how to do it, but it's really non-trivial. I like these diagrams, they look very cool. And I don't want to say what these mean Basically they complicated If you actually want to do computations on a surface code it takes a bit of work.

And we don't know how to do this with these asymptotically good codes. And actually in a project trying to find some way to do this, myself and Jérôme Gonneau, we proved some impossibility results. We actually said for certain families of these codes, there's certain ways to do these gates that are just never going to work. And this was a, it's not like we proved it would be impossible in general just kind of carving out some areas that aren't going to work other recent results had constructive results they showed some ways to encode or to do these operations on the quantum on these encoded quantum states but for worse codes.

And so why is this harder than the surface code basically this diagram kind of demonstrates it the surface code is very physically localized you look at this square and it is one logical qubit so if I do something to a physical qubit in that square, I'm only going to impact that one logical qubit. It kind of can't impact anything else because it's almost like I'm running a bunch of different codes in parallel.

To reach these asymptotically good rates, you need to have each physical qubit be kind of part of many logical qubits. So now if I modify the physical qubit, I run the risk that I'm actually modifying a lot more logical qubits than I want to. So this ability to kind of do any circuit I want and apply this gate to this qubit this gate to this other qubit, this gets much harder.

So overall, I'm not optimistic about the new codes. I sort of still think the surface code is going to be what gets us to factoring and breaking crypto. But all of that is kind of moving the bottom left of this chart, making the hardware better. Could we make the target closer? Could we have better algorithms?

# Potential Accelerators: Better Algorithms

So I wrote in 2021 that Shor's algorithm, it's mainly just modular exponentiation, so an asymptotic improvement is highly unlikely. And then two years later, Regev proved me very wrong and came up with a new quantum factoring algorithm.

Now, if you look at the total gate count, it's actually about the same as Shor's algorithm. But the nice thing is that it is basically a lot of independent shots. So you have a smaller circuit that you run many times. So Ekerå and Gartner's analyzed this and said, well, actually in practice, does this work out, certain subroutines of the algorithm might be more difficult. And what they found was, yeah, overall, we see the total number of operations is roughly 138,000 for the new factoring compared to about 6,000 for Ekerå and Hasuo's variant of Shor's algorithm.

So it is more computation overall, but you can split it into these runs, and each of the runs have fewer gates. So maybe you need actually less error correction because you have a shorter computation before you need to measure at the end. And they also showed that it tolerates runs of errors. Maybe the overall error correction might be lower. So this actually might get some improvement, but still, again, we kind of need to do some more work to figure out the specifics on this.

There have also been very recent other approaches here. So paper by Chevignard, Fouque, and Schrottenloher use the residue number system to reduce the logical qubit count so when i basically took their logical qubit count multiplied it by the overheads of the surface code and ignored you know all the gates in the the routing it was about 1.6 million physical qubits so probably that number would go up once we account for some of the extra overheads that surface code needs.

There's also some fast quantum integer multiplication that came out last year it seems to actually at the levels we might be using for something like Shor's algorithm this might actually offer an improvement unfortunately it's kind of independent of the last one because the residue number system is inherently using smaller integers so an asymptotically good multiplier is not going to do you much good because you've deliberately made the integers small so it probably these probably won't stack.

So overall this is really interesting but I'm kind of not expecting more than a factor of 10 cost reduction in factoring from these better algorithms.

# Potential Accelerators: Better Implementations

And then the last thing I want to mention is improvements in the implementations. So I think this is the third time that I'll talk about how to actually compute in a surface code. And so what this diagram is, is from a paper from 2019 that I'm guessing a lot of us have heard of, the how to factor RSA 2048 with 20 million noisy qubits. And they did this full stack estimation including saying, you know, in the surface code, what is each part of the quantum computer doing?

So this diagram is the X and Y axes are the actual planar layout of the surface code and the vertical axis is time So it sort of saying what are you doing to each of the qubits at each time step to make the computation happen And each of the red and pink pairs of boxes is doing one AND gate. So a huge portion of this is actually just dedicated to doing gates. So you need this sort of state factories and distillation and all sorts of complicated things. So you can see that it's a pretty big chunk of the qubit count is devoted to this.

And then as of last fall, there is a new technique to do this. So you can see on the second, the smallest big box is about half of the red box from last slide. This is doing T-state distillation. And they've improved it with this technique called catenation. And the diagram speaks for itself. They've shrunk it to this basically tiny column. So this could be a huge improvement on these circuit layouts because now your AND gates need a lot fewer qubits.

So not only could this drastically reduce the resources, now you can do other things because they really, in this resource estimation, looked at all the different ways you could do addition in a quantum computer and all the different techniques you could do and optimize them for the fact that every AND gate is such a huge cost in terms of both time and area. But now that it's a smaller cost, you can maybe start using techniques that optimize in other dimensions. And so the overall cost is going to improve also.

So when I look at all these improvements and I compare to this 2019 estimate that I've been using for all of my previous slides to say 2052, 2088, et cetera, the kind of steps of this full stack estimation in that paper obviously came before it was published, roughly 2017 to 2019, But now basically every one of them has had an improvement. And so the only thing left is the error corrected layout, kind of this full stack estimation of how all of these improvements might interact with each other and might build on each other and change the overall estimation.

So the chart I would like to show you would be to take the previous one I had based on this 2019 paper and move all these lines to the left. But I cannot show you this chart because I do not have the data for this. We need to make a new full stack resource estimation So in fact I asked Craig Gidney who made the last one do you know if anyone is working on this And it turns out yes So hopefully that comes out fairly soon and then we'll have to basically update this whole thing.

So in some sense, I was asked to talk one PQ crypto early, because probably by next year's PQ crypto, there will be a new estimate that says how to factor RSA 2048 with some number less than 20 million in some amount of time.

# Takeaways

So to put this all together, one of my main takeaways here, of course, is that qubits will need error correction. So when you try to look at the progress in quantum computing, pay attention to where error correction is, pay attention to physical versus logical qubits.

So our best estimate today to factor RSA 2048 is 20 million qubits, a factor of 200,000 from where we are today, but I think that that estimate is probably an overestimate. I think we will soon find ways to improve that. And overall, quantum computing is still immature, so basically expect the unexpected. There's probably a lot of unknown unknowns that may help or harm the progress in quantum computing.

Thank you.

# Q&A

**Q: Thanks a lot. I'm super taking RSA 2048 as an example but often you hear that ECC is going to be broken earlier than RSA so how about ECC 256 is that five years earlier or what's the timeline there? (Ruben)**

A: I expect it's earlier again we don't have a full stack estimation and so it's it's hard to say um when i've looked into this at the logical level it does seem to be easier basically because for the same reasons it's easier classically um that you're using some even though the arithmetic is a little bit more complicated the numbers you're using are eight times smaller And so the actual resources end up still being overall smaller. So I do expect ECC to break sooner, but I also don't expect it to be, you know, that much sooner. Right? Because again, to even get to factor to breaking either one of them, we need this exponential progress So if ECC is ten times easier to solve we maybe getting two three years of a gap.

**Q: The numbers and the examples that you showed on progress, they were mostly, well, actually almost all of them from big tech, right? So you cited Google, Microsoft, Amazon. So is there any evidence of maybe either public academic research or research that is happening somewhere outside of big tech behind the scenes that has an advantage to them? Or what would you expect on that? (Peter)**

A: um yeah so i mean it's an interesting kind of sociological question of why right now it's big tech is leading i think it's just this is really expensive to build that scale um and i think i get the sense that no one in academia has that kind of money to throw at it that say google or ibm has um so then the question is what about you know someone uh a three-letter agency somewhere uh i am if i was in charge of a three-letter agency i would absolutely not be putting money into this right now, I'd let Google and IBM do the hard work, and then when we get close, that's when I'd start scooping their talent. And the other thing is, I think BSI mentioned this, that there's certain kind of minerals and resources that to build with today's technology at the scale you'd need to factor, it would be kind of a noticeable chunk of the actual yearly production of these resources. And as far as I know, we just haven't seen that, that kind of, you know, these certain minerals are being disappeared. And so this makes me think that no one is actually doing this. So I don't expect anyone. I think genuinely Google has the best quantum computer in the world right now.

**Q: In your slides, you mentioned about quantum computing, and you said it's useless. And can you elaborate on that? In what way you meant by that?**

A: I guess I should. I don't want to pick on any one particular. Right now, all quantum computing is useless. So what I mean to say is that when I look at Google's result, they're building one surface code qubit, also useless. You can't do anything with one qubit that you couldn't do classically. But what I see there is that when you make more of that same logical qubit, eventually you reach a point where there are algorithms where we know there is quantum advantage. Random circuit sampling does not have this property. It's a problem kind of designed to show, basically showcase the quantum computer, to favor the quantum computer and show how good it is. So that is a task that has no practical application. So to say that that's what I mean by that particular problem is useless. And again, I don't want to be too critical. a good quantum computer that can do this is, if you believe in the entire project of quantum computing, definitely not useless. And I think the engineering challenges that were solved to get there are going to be useful to do other quantum computing things. But yeah, this is why I'm saying useless.

**Q: Sorry, one more question. So there are claims that they are building those quantum networks like use QKD so on such that they can allow to somehow connect the qubits together to help us to speed up the computations for factoring and so on is that true can you comment on that?**

A: yeah so this is so I think I don't think you do any QKD to connect the computers for factoring but you do a very similar thing of quantum teleportation so the idea is that maybe you won't ever be able to fit 20 million superconducting qubits into one fridge, you're kind of maxed out at some size of fridge, and then you want to move the quantum data between them. And to do that, you would do quantum teleportation. So yeah, there have been results in this, and this is something that has been improving over time as well that I didn't talk about at all. I think it's a little bit behind where it would need to be to keep up with where the chips are right now. So I'm sort of this I might throw this into one of the known known unknowns is what will this challenge how will we approach this challenge when we want to build the kind of 20 million qubit scale error quantum computer.

**Follow-up: This has been bothering me as well so because they are building photonic links between quantum computers and how do you even translate like a superconducting state into a photonic link without going classical?**

A: yeah so the way that you connect this and the physics of this I'm going to kind of hand wave so you have some you can make some sort of interaction between your superconductor photons in some way. I think what I've seen mostly is maybe doing this with trapped ions, but I don't think this is that much of a stretch to say that I've got one physical qubit, I can kind of move this to a photon one way or another. You know, I can have it have the one state emit a photon, the zero state not emit a photon or something like that, or emit a photon at a different phase. Once I can do that, then I kind of, I entangle my qubits, I turn one of them into a photon, i send that photon to another computer and then i do quantum teleportation so this is the the protocol that basically allows me to use if i have two entangled qubits in two different places then i can use classical communication to move the quantum state from one qubit over here to over there um so that's kind of what you need to do this this seems very plausible to me that this is a problem that can be solved.

**Q: You're talking about the progress on the quantum computing with all the public analogies right um it's possible some government agency are doing something behind the scene so what is your guesstimate that is it possible what's the probability that they will be going ahead of us breaking 2048 RSA and how many years?**

A: um yeah so i think i i kind of addressed that i i don't think they're that much ahead, basically because to me it doesn't make sense to try to be 20 years ahead of industry when you could let industry do the hard work and then just get a couple years ahead at the end. One thing that might be that they might be thinking of is kind of going at the other way so you know working on the algorithmic side and making the uh you know the factoring or the error correction better um so yeah i i don't have a good estimate of what will happen at the end right so maybe uh i i had this uh you know 2052 estimate which i i'm not super confident on so maybe you'd get a couple years ahead from a government agency um that's about all i could confidently say.

**Q: We have a person with the Debian laptop with a question he's been waiting for a while... You were referring about the limitation of LDPC's long connectivity and you were also skeptical about the solution over by the trap ion but wasn't there another new implementation with neutral atom qubits I think probably publishing earlier this year that they use the Rydberg I mean the Rydberg blocking interaction to resolve that i was i was kind of curious about your opinion with like are you also skeptical about that solution as well?**

A: uh yeah with the the Rydberg atoms that they're kind of shuffling them around yeah physically as i understand exactly i think it's a laser charge and there'll be resonance of the Rydberg to other Rydberg interaction things? Yeah, so that somehow feels more plausible to me because as I understand the technology, they're physically moving the qubits, like physically moving something in some way to make them interact. But I think what would be challenging about this is depending on how you have your parity check, you might need to be doing a lot of this at once or have one, you might have to do a lot of this movement for the error correction. And so I don't keep up with this enough to say for sure whether this is going to, the errors of all of that movement are going to kind of overwhelm what you're trying to do with the error correction. But yeah, that might be a more plausible path to kind of solve that first challenge.

**Q: By the way I curious about one thing So has there been like any of these sort of full stack experiments for factoring on other types of quantum computers like like I know that just implementing a basic gate making Toffoli only happen is different complexity in different kinds of systems?**

A: yeah so the more or less once you get into the surface code then whatever system got you there you kind of know what gates are going to be difficult and which ones are not going to be difficult. The kind of the way that you do Toffoli in a surface code is pretty much the same whatever technology has made your surface code. But then that's kind of saying, well, what got you to the surface code? That is kind of a superconducting, that technology favors superconductors because of this nearest neighbor connectivity. So potentially, if you did solve all these challenges for another code, you might end up in a situation where different gates are more challenging. so um yeah so specifically about like photonic quantum computers and topological ones because you know toffoli seems to be like hard on some of these things just to implement um yeah so i mean i don't know that you do it awfully physically in in in too many of these systems anyway um so for example in the surface code you you don't you're never physically doing a toffoli you're physically doing t gates bundling them together into toffoli um and uh so i think the probably would look the same on uh on the majorana qubits again in microsoft's paper where they were thinking of majorana qubits they were basically using the same resource estimates that were used for surface codes kind of thinking of the same kind of physical gates and then uh constructing them into logical gates in the same way. Okay, so basically the codes allow us to generalize into different kinds of implementations. That's interesting.

Any other questions? All right, let's thank the speaker. We'll have lunch next.

# Insights

- **Physical vs. Logical Qubits**: Distinction is pivotal; track logical qubit quality and count for scalable progress, as "logical qubits" in hype often denote non-surface-code encodings unfit for large-scale Shor.
- **Error Correction Thresholds**: Surface codes exponentially suppress errors via distance-d patches (~1000:1 physical-to-logical overhead at scale), optimal for 2D nearest-neighbor superconducting architectures; noise's two-dimensional nature (bit/phase flips) demands constant syndrome measurements (~kHz-MHz rates) with massive classical decoding (~100 GB/s throughput).
- **Qubit Count vs. Fidelity Tradeoff**: Post-threshold, scaling favors more mediocre qubits over perfect ones; surface code curves are steep, minimizing gains from 10^{-4} to 10^{-6} error rates.
- **Full-Stack Resource Estimation**: 2019 Gidney et al. benchmark (~20M qubits for RSA-2048) integrates layout, routing, distillation (e.g., T-states via catenation, shrinking AND gate overhead from dominant to minor), highlighting non-modular interactions; updates imminent.
- **Algorithmic Tweaks**: Regev's algorithm parallelizes into shallower circuits (138k total T-gates vs. 6k in optimized Shor, but tolerant to error bursts); residue number systems halve logical qubits (~1.6M physical post-overhead).
- **Hardware Diversity Risks/Rewards**: Superconductors lead (Google's real-time d=3-7 suppression); topological (Majorana braiding) or cat qubits (biased-noise suppression) promise leaps but face scaling skepticism; ion traps enable all-to-all but fidelity doubts at scale.
- **Hype Filters**: Prioritize surface code demos over qubit counts or random sampling; watch for cosmic rays, cryogenic scaling, photonic teleportation for modular fridges.
- **Timeline Sharpness**: Polynomial Shor masks exponential hardware needs, yielding abrupt transition (no advantage until ~2049, RSA breaks ~2052); ECC-256 likely 2-3 years earlier due to smaller fields.

# Transcription errors?

- Speaker name: "Sam Jakes" → corrected to "Sam Jaques" based on University of Waterloo quantum researcher known for factoring work.
- "Tzu Chongzhu" → "Zuchongzhi" (Chinese superconducting processor).
- "Cnots and t cultivation" → "CNOTs and T gates"; later "catenation" (T-state distillation technique, not "cultivation").
- "Eckera and Gartner" → "Ekerå and Gartner's"; "Ecker and Hashtag" → "Ekerå and Hasuo's".
- "Chevinier, Fouke, and schrotenlor" → "Chevignard, Fouque, and Schrottenloher".
- "Jérôme Guillaume" → "Jérôme Gonneau" (collaborator on LDPC fault-tolerance proofs).
- "PSI" → likely "PSIQ" or similar (possibly PsiQuantum); context suggests optimistic timeline predictor.
- "Number field CIV" → "number field sieve".
- "State factories" → likely "state factories" (magic state factories for Clifford+T universality).
- "Autophila" → "Toffoli" (quantum gate).
- Technical terms via context (e.g., LDPC, Majorana, Rydberg)


