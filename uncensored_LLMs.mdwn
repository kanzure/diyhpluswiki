Some uncensored LLMs:

huihui does some work in this area too: <https://huggingface.co/huihui-ai/Huihui-GLM-4.6V-Flash-abliterated-GGUF>

<https://ollama.com/huihui_ai/kimi-k2-abliterated>

<https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated>

<https://huggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera>

Dolphin org:

* <https://huggingface.co/dphn>
* <https://dphn.ai/> <https://x.com/dphnAI> <https://t.me/dphnAI>
* Eric Hartford <https://huggingface.co/ehartford> <https://x.com/cognitivecompai>
* <https://huggingface.co/dphn/Dolphin-Mistral-24B-Venice-Edition>
* [Dolphin X1 405b](https://blog.dphn.ai/405b/) 

Jinx-org:

* <https://huggingface.co/Jeol> <https://opdoop.github.io/>
* <https://huggingface.co/VivianKeith>
* <https://huggingface.co/Jinx-org/Jinx-Qwen3-235B-A22B-Thinking-2507>
* <https://huggingface.co/Jinx-org/Jinx-gpt-oss-20b>

### Abliteration

ablate (delete) weights pointing in the refusal direction as determined by simple PCA.
grimjim's [norm-preserving biprojected abliteration](https://huggingface.co/blog/grimjim/norm-preserving-biprojected-abliteration) method reduces distortion of weights not directly contributing to refusal, preserving sphericity, minimizing off-target effects on the model behavior, and often ending up with something more intelligent than the original, due to no longer suffering the alignment tax/safety tax.

- <https://huggingface.co/blog/mlabonne/abliteration> 
- <https://github.com/jim-plus/llm-abliteration/>
- [https://github.com/p-e-w/heretic](https://github.com/p-e-w/heretic): Fully automatic censorship removal for language models

<https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard> with "Uncensored" category.

<https://lobotomyq.com/> The Lobotomy Quotient ; Quantifying censorship levels across leading language models. Lower scores indicate more intact responses, higher scores reflect increased content filtering. but only 7 models benchmarked.

[Safety tax: Safety alignment makes your large reasoning models less reasonable](https://arxiv.org/abs/2503.00555)

[Jinx: Unlimited LLMs for probing alignment failures](https://arxiv.org/abs/2508.08243)


gpt-oss-120b unrestricted model service <https://pingu.audn.ai/> by reddit/u/ozgurozkan

<https://huggingface.co/BeaverAI>

<https://huggingface.co/TheDrummer> <https://www.patreon.com/TheDrummer>

<https://huggingface.co/Sao10K> <https://sao10k.carrd.co/> <https://patreon.com/Sao10K>

<https://huggingface.co/SicariusSicariiStuff>

<https://huggingface.co/Tarek07>

<https://huggingface.co/darkc0de/XortronCriminalComputingConfig>

<https://huggingface.co/collections/darkc0de/uncensored-champions>

<https://huggingface.co/yamatazen/LorablatedStock-12B>

<https://huggingface.co/TareksGraveyard/L3.3-TRP-BASE-80-70B>

need a good link for PonyXL i don't actually know where this is on huggingface

- <https://civitai.com> is where most image generation model tuning and development happens. most work has shifted to pony's successor illustrious (late 2025)

- there's also [Chroma](https://huggingface.co/lodestones/Chroma?not-for-all-audiences=true) [(HD)](https://huggingface.co/lodestones/Chroma1-HD)  for any furry porn imaginable.

<https://venice.ai/> an erik voorhees project?

<https://huggingface.co/ReadyArt/4.2.0-Broken-Tutu-24b>

<https://huggingface.co/mradermacher/Melinoe-30B-A3B-Thinking-i1-GGUF>

<https://huggingface.co/mradermacher/gemma-3-27b-it-abliterated-normpreserve-i1-GGUF>

<https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated>

<https://huggingface.co/YanLabs/gemma-3-27b-it-abliterated-normpreserve-v1-GGUF>

<https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-GGUF>

<https://huggingface.co/bartowski/mlabonne_gemma-3-27b-it-abliterated-GGUF>

<https://huggingface.co/ArliAI/gpt-oss-20b-Derestricted>

<https://huggingface.co/TheBloke/LLaMA2-13B-Tiefighter-GGUF>

